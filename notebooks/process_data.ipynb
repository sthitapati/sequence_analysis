{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Dict, Tuple, List, Union, Optional\n",
    "import pandas as pd\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadmat(filename: str) -> Dict:\n",
    "    '''\n",
    "    Load a .mat file and convert all mat-objects to nested dictionaries.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): The name of the .mat file to load.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the contents of the .mat file.\n",
    "    '''\n",
    "    # Load the .mat file\n",
    "    data = scipy.io.loadmat(filename, simplify_cells=True)\n",
    "    return data\n",
    "\n",
    "def import_bpod_data_files(input_path: str) -> Tuple[Dict[int, Dict], int, List[str], List[str]]:\n",
    "    '''\n",
    "    Load all '.mat' files in a given folder and convert them to Python format.\n",
    "\n",
    "    Parameters:\n",
    "    input_path (str): The path to the folder containing the '.mat' files.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[Dict[int, Dict], int, list, list]: A tuple containing the converted data, the number of sessions,\n",
    "    the list of file paths, and the list of file dates.\n",
    "    '''\n",
    "    # Get a list of all files in the input path\n",
    "    behav_path = sorted(os.listdir(input_path))\n",
    "    behav_data = {}  # Set up file dictionary\n",
    "    session_dates = []\n",
    "    sessions = 0  # For naming each data set within the main dictionary\n",
    "\n",
    "    # Loop through each file in the input path\n",
    "    for file in [f for f in behav_path if f.endswith('.mat') and os.stat(input_path + f).st_size > 200000]:\n",
    "        # Check if the file is not the weird hidden file\n",
    "        if file != '.DS_Store':\n",
    "            # Load the '.mat' file and add it to the dictionary\n",
    "            current_file = loadmat(input_path + file)\n",
    "            behav_data[sessions] = current_file\n",
    "            sessions += 1\n",
    "            session_dates.append(file[-19:-4])\n",
    "\n",
    "    return behav_data, sessions, behav_path, session_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All other helper functions used below\n",
    "\n",
    "def extract_poke_times(behavior_data: Dict) -> Tuple[List, List, List]:\n",
    "    \"\"\"\n",
    "    Extracts all port in/out times across the session for each port. \n",
    "    It aligns them to trial start timestamps so that the port in times \n",
    "    are across the whole session.\n",
    "\n",
    "    Parameters:\n",
    "    behavior_data (dict): The dictionary containing behavior data for the session.\n",
    "\n",
    "    Returns:\n",
    "    Tuple: Lists of all port in times, port out times, and corresponding port references.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store port in times, port out times and corresponding port references\n",
    "    all_port_in_times = []\n",
    "    all_port_out_times = []\n",
    "    all_port_references = []\n",
    "\n",
    "    # Iterate over each port\n",
    "    for port in range(1, 9):\n",
    "\n",
    "        # Initialize lists to store port in/out times for each port\n",
    "        port_in_times = []\n",
    "        port_out_times = []\n",
    "\n",
    "        # Iterate over each trial\n",
    "        for trial_index in range(behavior_data['SessionData']['nTrials']):\n",
    "\n",
    "            # Extract port in times\n",
    "            if f'Port{port}In' in behavior_data['SessionData']['RawEvents']['Trial'][trial_index]['Events']:\n",
    "                trial_start_timestamp = behavior_data['SessionData']['TrialStartTimestamp'][trial_index]\n",
    "                port_in_ts_offset = behavior_data['SessionData']['RawEvents']['Trial'][trial_index]['Events'][f'Port{port}In']\n",
    "                port_in_ts = trial_start_timestamp + port_in_ts_offset\n",
    "\n",
    "                # If port in timestamp is a single value, convert it to a list\n",
    "                if isinstance(port_in_ts, np.float64):\n",
    "                    port_in_ts = [port_in_ts]\n",
    "\n",
    "                # Add port in times to the list\n",
    "                port_in_times.extend(port_in_ts)\n",
    "\n",
    "            # Extract port out times\n",
    "            if f'Port{port}Out' in behavior_data['SessionData']['RawEvents']['Trial'][trial_index]['Events']:\n",
    "                trial_start_timestamp = behavior_data['SessionData']['TrialStartTimestamp'][trial_index]\n",
    "                port_out_ts_offset = behavior_data['SessionData']['RawEvents']['Trial'][trial_index]['Events'][f'Port{port}Out']\n",
    "                port_out_ts = trial_start_timestamp + port_out_ts_offset\n",
    "\n",
    "                # If port out timestamp is a single value, convert it to a list\n",
    "                if isinstance(port_out_ts, np.float64):\n",
    "                    port_out_ts = [port_out_ts]\n",
    "\n",
    "                # Add port out times to the list\n",
    "                port_out_times.extend(port_out_ts)\n",
    "\n",
    "        # Check if the number of port in times and port out times are equal\n",
    "        # If not, apply error check and fix\n",
    "        if len(port_in_times) != len(port_out_times):\n",
    "            port_in_times, port_out_times = error_check_and_fix(port_in_times, port_out_times)\n",
    "\n",
    "        # Add port in times, port out times and port references to the overall lists\n",
    "        all_port_references.extend([port] * len(port_in_times))\n",
    "        all_port_in_times.extend(port_in_times)\n",
    "        all_port_out_times.extend(port_out_times)\n",
    "\n",
    "    return all_port_in_times, all_port_out_times, all_port_references\n",
    "\n",
    "def error_check_and_fix(port_in_times: List, port_out_times: List) -> Tuple[List, List]:\n",
    "    \"\"\"\n",
    "    Checks and corrects mismatches in the length of port in and port out times lists.\n",
    "    If lengths are unequal, 'nan' is inserted at the appropriate position or appended to the shorter list.\n",
    "\n",
    "    Parameters:\n",
    "    port_in_times (List): The list of port in times.\n",
    "    port_out_times (List): The list of port out times.\n",
    "\n",
    "    Returns:\n",
    "    Tuple: The corrected port in times and port out times lists.\n",
    "    \"\"\"\n",
    "    # Initialize fixed flag as False\n",
    "    fixed = False\n",
    "\n",
    "    # If the lengths of port in times and port out times lists are not equal\n",
    "    if len(port_in_times) != len(port_out_times):\n",
    "\n",
    "        # If port in times list is longer than port out times list\n",
    "        if len(port_in_times) > len(port_out_times):\n",
    "            # Iterate over each item in the port out times list\n",
    "            for i in range(len(port_out_times)):\n",
    "                # If the port out time is later than the next port in time\n",
    "                if port_out_times[i] >= port_in_times[i+1]:\n",
    "                    # Insert a 'nan' at this position in the port out times list\n",
    "                    port_out_times.insert(i, 'nan')\n",
    "                    fixed = True\n",
    "\n",
    "            # If the issue wasn't fixed by the above process, append 'nan' to port out times list\n",
    "            if len(port_in_times) > len(port_out_times) and not fixed:\n",
    "                port_out_times.append('nan')\n",
    "\n",
    "        # If port out times list is longer than port in times list\n",
    "        elif len(port_out_times) > len(port_in_times):\n",
    "            # Iterate over each item in the port in times list\n",
    "            for i in range(len(port_in_times)):\n",
    "                # If the port in time is later than or equal to the port out time\n",
    "                if port_in_times[i] >= port_out_times[i]:\n",
    "                    # Insert a 'nan' at this position in the port in times list\n",
    "                    port_in_times.insert(i, 'nan')\n",
    "                    fixed = True\n",
    "\n",
    "            # If the issue wasn't fixed by the above process, append 'nan' to port in times list\n",
    "            if len(port_out_times) > len(port_in_times) and not fixed:\n",
    "                port_in_times.append('nan')\n",
    "\n",
    "    # If the lengths of port in times and port out times lists are still not equal\n",
    "    if len(port_in_times) != len(port_out_times):\n",
    "        print('Dropped event not fixed!!!!')\n",
    "\n",
    "    return port_in_times, port_out_times\n",
    "\n",
    "def remove_dropped_in_events(port_in_times: List, port_out_times: List, port_references: List) -> Tuple[List, List, List]:\n",
    "    \"\"\"\n",
    "    Cleans up the data by removing 'nan' values from the lists of port in times, port out times, and port references.\n",
    "\n",
    "    Parameters:\n",
    "    port_in_times (List): The list of port in times.\n",
    "    port_out_times (List): The list of port out times.\n",
    "    port_references (List): The list of port references.\n",
    "\n",
    "    Returns:\n",
    "    Tuple: The cleaned port in times, port out times, and port references lists.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a reversed list of indexes to remove in order to avoid index shifting during removal\n",
    "    indexes_to_remove = [i for i, time in enumerate(port_in_times) if time == 'nan'][::-1]\n",
    "\n",
    "    for index in indexes_to_remove:\n",
    "        # Remove 'nan' entries from each list\n",
    "        del port_in_times[index]\n",
    "        del port_out_times[index]\n",
    "        del port_references[index]\n",
    "\n",
    "    return port_out_times, port_in_times, port_references\n",
    "\n",
    "def sort_by_time(port_in_times: List, port_out_times: List, port_references: List) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Sorts the data by port in times. If an out time is missing, it will be appended with 'nan'.\n",
    "\n",
    "    Parameters:\n",
    "    port_in_times (List): The list of port in times.\n",
    "    port_out_times (List): The list of port out times.\n",
    "    port_references (List): The list of port references.\n",
    "\n",
    "    Returns:\n",
    "    Tuple: The sorted port in times, port out times, and port references.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the indices that would sort the in times\n",
    "    sort_indices = np.argsort(port_in_times)\n",
    "\n",
    "    # Apply the sorted indices to each list and convert them to numpy arrays\n",
    "    sorted_in_times = np.array(port_in_times, dtype=float)[sort_indices]\n",
    "    sorted_references = np.array(port_references)[sort_indices]\n",
    "    \n",
    "    # Check if the number of out times matches the number of sorted indices\n",
    "    if len(sort_indices) == len(port_out_times):\n",
    "        sorted_out_times = np.array(port_out_times, dtype=float)[sort_indices]\n",
    "    else:\n",
    "        # If they don't match, append a 'nan' to the out times before sorting\n",
    "        sorted_out_times = np.array(port_out_times + [np.nan], dtype=float)[sort_indices]\n",
    "\n",
    "    return sorted_in_times, sorted_out_times, sorted_references\n",
    "\n",
    "def extract_reward_timestamps(behavior_data: Dict) -> List[float]:\n",
    "    '''\n",
    "    Extracts all reward timestamps across a session for each port.\n",
    "\n",
    "    Parameters:\n",
    "    behavior_data (Dict): The behavioral data dictionary.\n",
    "\n",
    "    Returns:\n",
    "    List[float]: A list containing all the reward timestamps for the session.\n",
    "    '''\n",
    "    # Initialize list to store all reward timestamps\n",
    "    reward_timestamps = []\n",
    "\n",
    "    # Iterate over each trial in the session\n",
    "    for trial in range(behavior_data['SessionData']['nTrials']):\n",
    "        \n",
    "        # Check if the 'Reward' event exists in the trial data\n",
    "        if 'Reward' in behavior_data['SessionData']['RawEvents']['Trial'][trial]['States']:\n",
    "            \n",
    "            # Calculate the timestamp of the reward event relative to the start of the trial\n",
    "            trial_start_timestamp = behavior_data['SessionData']['TrialStartTimestamp'][trial]\n",
    "            reward_time_offset = behavior_data['SessionData']['RawEvents']['Trial'][trial]['States']['Reward'][0]\n",
    "            \n",
    "            # Convert the reward timestamp to the session timeline\n",
    "            reward_timestamp = trial_start_timestamp + reward_time_offset\n",
    "            \n",
    "            # Add the reward timestamp to the list of reward timestamps\n",
    "            reward_timestamps.append(reward_timestamp)\n",
    "\n",
    "    return reward_timestamps\n",
    "\n",
    "\n",
    "def find_rewarded_event_indices(sorted_in_timestamps: List[float], \n",
    "                                sorted_port_references: List[int], \n",
    "                                reward_timestamps: List[float]) -> List[int]:\n",
    "    '''\n",
    "    Identifies the indices of rewarded events.\n",
    "\n",
    "    Parameters:\n",
    "    sorted_in_timestamps (List[float]): List of sorted poke in timestamps.\n",
    "    sorted_port_references (List[int]): List of port references corresponding to the poke in timestamps.\n",
    "    reward_timestamps (List[float]): List of reward timestamps.\n",
    "\n",
    "    Returns:\n",
    "    List[int]: Indices of rewarded events in sorted_in_timestamps and sorted_port_references.\n",
    "    '''\n",
    "\n",
    "    rewarded_event_indices = []  # Initialize the list to store indices of rewarded events\n",
    "    reward_index = 0  # Initialize reward index counter\n",
    "\n",
    "    # Iterate over sorted port references with their indices\n",
    "    for event_index, port_number in enumerate(sorted_port_references):\n",
    "        \n",
    "        # Check if port number is 7 and there are reward timestamps left to process\n",
    "        if port_number == 7 and reward_index < len(reward_timestamps):\n",
    "            \n",
    "            # Skip NaN timestamps\n",
    "            while np.isnan(reward_timestamps[reward_index]):\n",
    "                reward_index += 1\n",
    "\n",
    "                # If there are no more reward timestamps, exit the loop\n",
    "                if reward_index >= len(reward_timestamps):\n",
    "                    break\n",
    "\n",
    "            # If there are still reward timestamps left, check if the in time is greater than or equal to the current reward timestamp\n",
    "            if reward_index < len(reward_timestamps) and sorted_in_timestamps[event_index] >= reward_timestamps[reward_index]:\n",
    "                \n",
    "                # If so, record the event index as a rewarded event\n",
    "                rewarded_event_indices.append(event_index)\n",
    "                \n",
    "                # And move on to the next reward timestamp\n",
    "                reward_index += 1\n",
    "\n",
    "    return rewarded_event_indices\n",
    "\n",
    "def align_trigger_to_index(triggers: List[float], \n",
    "                           trigger_indices: List[int], \n",
    "                           all_timestamps: List[float]) -> List[Union[float, str]]:\n",
    "    '''\n",
    "    Aligns triggers to their corresponding indices in the timestamp array.\n",
    "\n",
    "    Parameters:\n",
    "    triggers (List[float]): List of trigger timestamps.\n",
    "    trigger_indices (List[int]): List of indices corresponding to trigger timestamps.\n",
    "    all_timestamps (List[float]): List of all timestamps.\n",
    "\n",
    "    Returns:\n",
    "    List[Union[float, str]]: Array with triggers aligned to their indices, \n",
    "                              and 'NaN' for all other indices.\n",
    "    '''\n",
    "\n",
    "    # Initialize output array with 'NaN' for all indices\n",
    "    aligned_triggers = ['NaN'] * len(all_timestamps)\n",
    "    \n",
    "    # Assign trigger values to their corresponding indices\n",
    "    for trigger_value, trigger_index in zip(triggers, trigger_indices):\n",
    "        aligned_triggers[trigger_index] = trigger_value\n",
    "\n",
    "    return aligned_triggers\n",
    "\n",
    "def extract_trial_timestamps(behavior_data):\n",
    "    \"\"\"\n",
    "    Extracts trial timestamps from behavioral data.\n",
    "\n",
    "    Args:\n",
    "        behavior_data: The complete behavioral data dictionary.\n",
    "\n",
    "    Returns:\n",
    "        A list of trial timestamps.\n",
    "    \"\"\"\n",
    "    trial_timestamps = []\n",
    "    for trial in range(behavior_data['SessionData']['nTrials']):\n",
    "        trial_start_timestamp = behavior_data['SessionData']['TrialStartTimestamp'][trial]\n",
    "        trial_timestamps.append(trial_start_timestamp)\n",
    "    return trial_timestamps\n",
    "\n",
    "\n",
    "def extract_trial_end_times(behavior_data):\n",
    "    \"\"\"\n",
    "    Extracts trial end times from behavioral data.\n",
    "\n",
    "    Args:\n",
    "        behavior_data: The complete behavioral data dictionary.\n",
    "\n",
    "    Returns:\n",
    "        A list of trial end times.\n",
    "    \"\"\"\n",
    "\n",
    "    all_end_times = []\n",
    "    for trial in range(behavior_data['SessionData']['nTrials']):\n",
    "        if 'ExitSeq' in behavior_data['SessionData']['RawEvents']['Trial'][trial]['States']:\n",
    "            trial_start_timestamp = behavior_data['SessionData']['TrialStartTimestamp'][trial]\n",
    "            exit_time_offset = behavior_data['SessionData']['RawEvents']['Trial'][trial]['States']['ExitSeq'][-1]\n",
    "            end_times = trial_start_timestamp + exit_time_offset\n",
    "            all_end_times.append(end_times)\n",
    "    return all_end_times\n",
    "\n",
    "    trial_timestamps = []\n",
    "    for trial in range(behavior_data['SessionData']['nTrials']):\n",
    "        trial_start_timestamp = behavior_data['SessionData']['TrialStartTimestamp'][trial]\n",
    "        trial_timestamps.append(trial_start_timestamp)\n",
    "    return trial_timestamps\n",
    "\n",
    "\n",
    "def extract_trial_end_times(behavior_data):\n",
    "    \"\"\"\n",
    "    Extracts trial end times from behavioral data.\n",
    "\n",
    "    Args:\n",
    "        behavior_data: The complete behavioral data dictionary.\n",
    "\n",
    "    Returns:\n",
    "        A list of trial end times.\n",
    "    \"\"\"\n",
    "\n",
    "    all_end_times = []\n",
    "    for trial in range(behavior_data['SessionData']['nTrials']):\n",
    "        if 'ExitSeq' in behavior_data['SessionData']['RawEvents']['Trial'][trial]['States']:\n",
    "            trial_start_timestamp = behavior_data['SessionData']['TrialStartTimestamp'][trial]\n",
    "            exit_time_offset = behavior_data['SessionData']['RawEvents']['Trial'][trial]['States']['ExitSeq'][-1]\n",
    "            end_times = trial_start_timestamp + exit_time_offset\n",
    "            all_end_times.append(end_times)\n",
    "        else:\n",
    "            all_end_times.append('NaN')\n",
    "\n",
    "    return all_end_times\n",
    "\n",
    "def determine_trial_id(sorted_port_in_times: np.ndarray, trial_end_timestamps: List[float]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Determines the trial id for each port event.\n",
    "\n",
    "    Args:\n",
    "        sorted_port_in_times: Sorted numpy array of times when a port event starts.\n",
    "        trial_end_timestamps: List of times when each trial ends.\n",
    "\n",
    "    Returns:\n",
    "        A list of trial ids, one for each port event. \n",
    "        The id is determined by comparing the port event time with the trial end times.\n",
    "    \"\"\"\n",
    "\n",
    "    trial_ids = []\n",
    "    current_trial = 1\n",
    "    for current_time in sorted_port_in_times:\n",
    "        if current_trial > len(trial_end_timestamps):\n",
    "            trial_ids.append(current_trial)\n",
    "        elif current_time <= trial_end_timestamps[current_trial - 1]:\n",
    "            trial_ids.append(current_trial)\n",
    "        else:\n",
    "            current_trial += 1\n",
    "            trial_ids.append(current_trial)\n",
    "    return trial_ids\n",
    "\n",
    "\n",
    "def find_trial_start_indices(trial_ids):\n",
    "    \"\"\"\n",
    "    Determines the start indices for each trial.\n",
    "\n",
    "    Args:\n",
    "        trial_ids: List of trial ids for each port event.\n",
    "\n",
    "    Returns:\n",
    "        A list of start indices for each trial.\n",
    "    \"\"\"\n",
    "\n",
    "    trial_start_indices = [0]\n",
    "    for index, trial_id in enumerate(trial_ids[1:], 1):  # start enumerating from 1\n",
    "        if trial_id != trial_ids[index-1]:\n",
    "            trial_start_indices.append(index)\n",
    "    return trial_start_indices\n",
    "\n",
    "\n",
    "def align_trial_start_end_timestamps(\n",
    "    trial_ids: list, \n",
    "    trial_start_indices: list, \n",
    "    trial_start_timestamps: list\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Aligns trial start and end timestamps.\n",
    "\n",
    "    Args:\n",
    "        trial_ids: List of trial IDs, the length of which defines the iteration count.\n",
    "        trial_start_indices: List of indices where a new trial starts in the list of trial IDs.\n",
    "        trial_start_timestamps: List of timestamps corresponding to each start index.\n",
    "\n",
    "    Returns:\n",
    "        A list of aligned trial start times. If there's an index without a corresponding timestamp, numpy's nan is appended.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(trial_ids) < max(len(trial_start_indices), len(trial_start_timestamps)):\n",
    "        raise ValueError(\"Length of trial_ids cannot be less than either trial_start_indices or trial_start_timestamps.\")\n",
    "\n",
    "    aligned_trial_timestamps = []\n",
    "    counter = 0\n",
    "    for i in range(len(trial_ids)):\n",
    "        if counter + 1 < len(trial_start_indices) and i == trial_start_indices[counter+1]:\n",
    "            counter += 1\n",
    "        if counter < len(trial_start_timestamps):\n",
    "            aligned_trial_timestamps.append(trial_start_timestamps[counter])\n",
    "        else:\n",
    "            aligned_trial_timestamps.append(np.nan)\n",
    "\n",
    "    if len(trial_start_timestamps) != len(trial_start_indices):\n",
    "        difference = abs(len(trial_start_timestamps) - len(trial_start_indices))\n",
    "        if difference > 5:\n",
    "            warnings.warn(f\"Difference between trial_start_timestamps and trial_start_indices exceeds 5: {difference}\")\n",
    "\n",
    "    return aligned_trial_timestamps\n",
    "\n",
    "def find_trial_start_and_poke1_camera_indices(camera_trigger_states: np.ndarray) -> Tuple[List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Find indices in the camera timestamps where the trial starts and the first poke happens.\n",
    "\n",
    "    Args:\n",
    "        camera_trigger_states (np.ndarray): Array of trigger states from the camera.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[int], List[int]]: Lists of indices where trial starts and the first poke happens.\n",
    "    \"\"\"\n",
    "    ttl_change_indices = list(np.where(np.roll(camera_trigger_states, 1) != camera_trigger_states)[0])\n",
    "    if ttl_change_indices[0] == 0:\n",
    "        ttl_change_indices = ttl_change_indices[1:]\n",
    "\n",
    "    poke1_camera_indices = ttl_change_indices[1::2]\n",
    "    trial_start_camera_indices = ttl_change_indices[0::2]\n",
    "\n",
    "    return trial_start_camera_indices, poke1_camera_indices\n",
    "\n",
    "\n",
    "def generate_aligned_trial_end_camera_timestamps(trial_start_camera_indices: List[int], trial_ids: List[int], trial_start_indices: List[int], camera_timestamps: np.ndarray) -> List[Union[float, str]]:\n",
    "    \"\"\"\n",
    "    Generate aligned timestamps for the end of trials based on camera timestamps.\n",
    "\n",
    "    Args:\n",
    "        trial_start_camera_indices (List[int]): List of indices where each trial starts.\n",
    "        trial_ids (List[int]): List of trial ids for each port event.\n",
    "        trial_start_indices (List[int]): List of start indices for each trial.\n",
    "        camera_timestamps (np.ndarray): Array of camera timestamps.\n",
    "\n",
    "    Returns:\n",
    "        List[Union[float, str]]: List of aligned trial end timestamps.\n",
    "    \"\"\"\n",
    "    end_indices = [item for index, item in enumerate(trial_start_camera_indices) if index > 0]\n",
    "    aligned_trial_end_timestamps = align_trial_start_end_timestamps(trial_ids, trial_start_indices, camera_timestamps[end_indices])\n",
    "\n",
    "    last_trial_length = len(trial_ids) - trial_start_indices[-1]\n",
    "    if len(aligned_trial_end_timestamps) == len(trial_ids):\n",
    "        del aligned_trial_end_timestamps[-last_trial_length:]\n",
    "\n",
    "    aligned_trial_end_timestamps += ['NaN'] * last_trial_length\n",
    "    return aligned_trial_end_timestamps\n",
    "\n",
    "\n",
    "def align_firstpoke_camera_timestamps(trial_ids: List[int], trial_start_indices: List[int], trial_start_timestamps: List[float], all_port_references_sorted: List[float]) -> List[Union[float, str]]:\n",
    "    \"\"\"\n",
    "    Align the timestamps of the first poke with the camera timestamps.\n",
    "\n",
    "    Args:\n",
    "        trial_ids (List[int]): List of trial ids for each port event.\n",
    "        trial_start_indices (List[int]): List of start indices for each trial.\n",
    "        trial_start_timestamps (List[float]): List of trial start timestamps.\n",
    "        all_port_references_sorted (List[float]): Sorted list of all port references.\n",
    "\n",
    "    Returns:\n",
    "        List[Union[float, str]]: List of aligned first poke timestamps.\n",
    "    \"\"\"\n",
    "    trial_timestamps_aligned = []\n",
    "    counter = 0\n",
    "    for index, item in enumerate(trial_ids):\n",
    "        if all_port_references_sorted[index] == 2.0:\n",
    "            if item > counter:\n",
    "                counter += 1\n",
    "                if len(trial_start_timestamps) != counter - 1:\n",
    "                    trial_timestamps_aligned.append(trial_start_timestamps[counter-1])\n",
    "                else:\n",
    "                    trial_timestamps_aligned.append('NaN')\n",
    "            else:\n",
    "                trial_timestamps_aligned.append('NaN')\n",
    "        else:\n",
    "            trial_timestamps_aligned.append('NaN')\n",
    "    return trial_timestamps_aligned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### handle data for optogenetics experiments\n",
    "\n",
    "def handle_opto_stim_data(behavior_data, trial_settings, session_index, trial_ids):\n",
    "    \"\"\"\n",
    "    Handles the optostim data. If optostim was enabled, creates a dataframe of optostim settings and\n",
    "    aligns optostim trial data to the trial data. If optostim was not enabled, creates a list of 'NaN' values.\n",
    "    If StimPoke was set to 5, includes additional variables in the settings dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    behavior_data (dict): The behavior data dictionary.\n",
    "    trial_settings (dict): The trial settings dictionary.\n",
    "    session_index (int): The current session index.\n",
    "    trial_ids (list): List of trial ids.\n",
    "\n",
    "    Returns:\n",
    "    optotrials_aligned (list): The list of aligned optostim trial data.\n",
    "    optotrials_port_aligned (list): The list of aligned optostim port data.\n",
    "    \"\"\"\n",
    "    if trial_settings['GUI']['OptoStim'] == 1:\n",
    "        # Create opto settings as a dataframe\n",
    "        opto_settings = pd.DataFrame({\n",
    "            'StimPoke': [trial_settings['GUI']['StimPoke']],\n",
    "            'PulsePower': [trial_settings['GUI']['PulsePower']],\n",
    "            'OptoChance': [trial_settings['GUI']['OptoChance']],\n",
    "            'PulseDuration': [trial_settings['GUI']['PulseDuration']],\n",
    "            'PulseInterval': [trial_settings['GUI']['PulseInterval']],\n",
    "            'TrainDuration': [trial_settings['GUI']['TrainDuration']],\n",
    "            'TrainDelay': [trial_settings['GUI']['TrainDelay']] if 'TrainDelay' in trial_settings['GUI'] else [None]\n",
    "        })\n",
    "\n",
    "        # Pull out optotrials from data if available\n",
    "        optotrials = behavior_data[session_index]['SessionData']['SessionVariables']['OptoStim']\n",
    "\n",
    "        # Align these to dataframe\n",
    "        executed_optotrials = optotrials[0:trial_ids[-1]]\n",
    "        optotrials_aligned = align_opto_data(trial_ids, executed_optotrials)\n",
    "\n",
    "        # Determine stimulated port\n",
    "        if trial_settings['GUI']['StimPoke'] == 5:\n",
    "            port_stimulated_data = behavior_data[session_index]['SessionData']['SessionVariables']['PortStimulated']\n",
    "            optotrials_port = np.where(port_stimulated_data == 1)[1] + 1  # Adding 1 to match port numbers 1 through 4\n",
    "        else:\n",
    "            optotrials_port = [trial_settings['GUI']['StimPoke']] * len(trial_ids)\n",
    "\n",
    "        # align ports to dataframe\n",
    "        optotrials_port_aligned = align_data_to_trial_ids(trial_ids, optotrials_port)\n",
    "    else:\n",
    "        # No optostim so fill this column with NaNs\n",
    "        optotrials_aligned = ['NaN'] * len(trial_ids)\n",
    "        optotrials_port_aligned = ['NaN'] * len(trial_ids)\n",
    "     \n",
    "    return optotrials_aligned, optotrials_port_aligned\n",
    "\n",
    "def align_data_to_trial_ids(trial_ids: List[int], data: List[int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    This function aligns the given data according to the trial ids.\n",
    "\n",
    "    Args:\n",
    "        trial_ids (List[int]): The list of trial ids.\n",
    "        data (List[int]): The list of data to align.\n",
    "\n",
    "    Returns:\n",
    "        List[int]: The list of aligned data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the counter for executed trials and list for aligned trials\n",
    "    data_counter = 0\n",
    "    aligned_data = []\n",
    "\n",
    "    # Iterate over the list of trial ids\n",
    "    for index, trial_id in enumerate(trial_ids):\n",
    "        # For the first trial, simply append the first data item\n",
    "        if index == 0:\n",
    "            aligned_data.append(data[data_counter])\n",
    "        else:\n",
    "            # If the current trial id is same as previous one, append the same data item\n",
    "            if trial_id == trial_ids[index-1]:\n",
    "                if data_counter < len(data):\n",
    "                    aligned_data.append(data[data_counter])\n",
    "                else:\n",
    "                    aligned_data.append(float('nan'))\n",
    "            else:\n",
    "                # If the trial id has changed, increment the counter\n",
    "                data_counter += 1\n",
    "                # Check if data_counter has not exceeded the length of data\n",
    "                if data_counter < len(data):\n",
    "                    # Append the next data item\n",
    "                    aligned_data.append(data[data_counter])\n",
    "                else:\n",
    "                    # If data_counter has exceeded the length of data, append NaN or any other suitable value\n",
    "                    aligned_data.append(float('nan'))\n",
    "\n",
    "    return aligned_data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_camera_timestamps(session_date: str, camera_directory: str, animal_id: str) -> Tuple[bool, Union[str, None]]:\n",
    "    \"\"\"\n",
    "    Searches for timestamp files for a given animal and session date in the camera directory.\n",
    "    \n",
    "    Args:\n",
    "        session_date (str): The date of the session, in 'yyyymmddHHMMSS' format.\n",
    "        camera_directory (str): The path to the directory where camera files are stored.\n",
    "        animal_id (str): The ID of the animal.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[bool, Union[str, None]]: A tuple with a boolean indicating whether the timestamp file exists,\n",
    "        and the path to the timestamp file, if it exists. If no timestamp file is found, the path is None.\n",
    "    \"\"\"\n",
    "    # Format the session date in 'ddmmyy' format\n",
    "    formatted_date = session_date[6:8] + session_date[4:6] + session_date[2:4]\n",
    "\n",
    "    timestamps_exist = False\n",
    "    timestamp_file_path = None\n",
    "\n",
    "    # Check if the camera directory for the animal exists\n",
    "    animal_camera_directory = os.path.join(camera_directory, animal_id)\n",
    "    if not os.path.isdir(animal_camera_directory):\n",
    "        return timestamps_exist, timestamp_file_path\n",
    "\n",
    "    # Check if there is a directory for the session date\n",
    "    if formatted_date in os.listdir(animal_camera_directory):\n",
    "        session_date_directory = os.path.join(animal_camera_directory, formatted_date)\n",
    "\n",
    "        # Look for timestamp file in the session date directory\n",
    "        for filename in os.listdir(session_date_directory):\n",
    "            # Check if the file is a csv file\n",
    "            if filename.endswith('.csv'):\n",
    "                # Extract timestamp from filename\n",
    "                file_timestamp = filename[-12:-4].replace(\"_\", \"\")\n",
    "\n",
    "                # Check if the file was created before the session start time\n",
    "                if int(file_timestamp) < int(session_date[9:15]):\n",
    "                    timestamps_exist = True\n",
    "                    timestamp_file_path = os.path.join(session_date_directory, filename)\n",
    "                    break\n",
    "\n",
    "    return timestamps_exist, timestamp_file_path\n",
    "\n",
    "\n",
    "### Timestamp preprocessing:\n",
    "\n",
    "def load_camera_timestamps(input_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load camera timestamps from a file.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Path to the file containing camera timestamps.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe containing camera timestamps.\n",
    "    \"\"\"\n",
    "    camera_timestamps = pd.read_csv(input_path, sep=' ', header=None, names=['Trigger', 'Timestamp', 'blank'], index_col=2)\n",
    "    del camera_timestamps['blank']\n",
    "    return camera_timestamps\n",
    "\n",
    "def convert_time(time: int) -> float:\n",
    "    \"\"\"\n",
    "    Convert the time from a timestamp into seconds.\n",
    "\n",
    "    Args:\n",
    "        time (int): The timestamp to be converted.\n",
    "\n",
    "    Returns:\n",
    "        float: The timestamp converted into seconds.\n",
    "    \"\"\"\n",
    "    cycle1 = (time >> 12) & 0x1FFF\n",
    "    cycle2 = (time >> 25) & 0x7F\n",
    "    seconds = cycle2 + cycle1 / 8000.\n",
    "    return seconds\n",
    "\n",
    "def uncycle(time: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Uncycle the time array.\n",
    "\n",
    "    Args:\n",
    "        time (np.ndarray): Time array to be uncycled.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Uncycled time array.\n",
    "    \"\"\"\n",
    "    cycles = np.insert(np.diff(time) < 0, 0, False)\n",
    "    cycle_index = np.cumsum(cycles)\n",
    "    return time + cycle_index * 128\n",
    "\n",
    "def convert_uncycle_timestamps(camera_timestamps: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert the timestamps into seconds and uncycle them.\n",
    "\n",
    "    Args:\n",
    "        camera_timestamps (pd.DataFrame): DataFrame containing camera timestamps.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Uncycled timestamps in seconds.\n",
    "    \"\"\"\n",
    "    timestamps_s = []\n",
    "    for index, row in camera_timestamps.iterrows():\n",
    "        if row.Trigger > 0: \n",
    "            timestamp_new = convert_time(camera_timestamps.at[index, 'Timestamp'])\n",
    "            timestamps_s.append(timestamp_new)\n",
    "        else:    \n",
    "            raise ValueError('Timestamps are broken')\n",
    "    uncycled_timestamps = uncycle(timestamps_s)\n",
    "    uncycled_timestamps = uncycled_timestamps - uncycled_timestamps[0]  # make first timestamp 0 and the others relative to this \n",
    "    return uncycled_timestamps\n",
    "\n",
    "def check_timestamps(timestamps: np.ndarray, frame_rate: int) -> None:\n",
    "    \"\"\"\n",
    "    Check for dropped frames in the timestamps.\n",
    "\n",
    "    Args:\n",
    "        timestamps (np.ndarray): Array of timestamps.\n",
    "        frame_rate (int): Frame rate in frames per second.\n",
    "    \"\"\"\n",
    "    frame_gaps = 1 / np.diff(timestamps)\n",
    "    frames_dropped = np.sum((frame_gaps < frame_rate - 5) | (frame_gaps > frame_rate + 5))\n",
    "    print('Frames dropped = ' + str(frames_dropped))\n",
    "    plt.suptitle('Frame rate = ' + str(frame_rate) + 'fps', color = 'red')\n",
    "    plt.hist(frame_gaps, bins=100)\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Number of frames')\n",
    "\n",
    "def find_trigger_states(camera_timestamps_raw: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Determine the trigger states from the raw camera timestamps.\n",
    "\n",
    "    Args:\n",
    "        camera_timestamps_raw (pd.DataFrame): DataFrame containing raw camera timestamps.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of trigger states.\n",
    "    \"\"\"\n",
    "    down_state = camera_timestamps_raw['Trigger'][0]\n",
    "    down_state_times = np.where(camera_timestamps_raw['Trigger'] == down_state)\n",
    "    triggers_temp = np.ones(len(camera_timestamps_raw['Trigger']))\n",
    "    triggers_temp[down_state_times] = 0\n",
    "    return triggers_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO check if all functions work with a single session and then think about how to combine them for multiple sessions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_ids = [\"EJT244\",\"SP110\", \"SP111\"]\n",
    "input_directory = '/home/sthitapati/Documents/sequence_data/bpod_raw_data/'\n",
    "output_directory = '/home/sthitapati/Documents/sequence_data/output/'\n",
    "camera_directory =  '/home/sthitapati/Documents/sequence_data/FlyCap_SeqTracking/'\n",
    "replace_existing = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_animal_data(\n",
    "    animal_ids: List[str], \n",
    "    input_directory: str, \n",
    "    output_directory: str, \n",
    "    camera_directory: Optional[str] = None, \n",
    "    replace_existing: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Function to process data for each animal and each session.\n",
    "\n",
    "    Args:\n",
    "        animal_ids (List[str]): List of animal IDs.\n",
    "        input_directory (str): Directory containing raw behavioral data for each animal.\n",
    "        output_directory (str): Directory where processed data will be saved.\n",
    "        camera_directory (Optional[str]): Directory containing the camera timestamp files for each animal, if available.\n",
    "        replace_existing (bool): If True, existing processed data will be replaced. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate over each animal by its index and ID\n",
    "    for animal_index, current_animal_id in enumerate(animal_ids):\n",
    "        print ('Processing data for: ' + current_animal_id)\n",
    "\n",
    "        # Construct the path for the current animal's data\n",
    "        current_input_path = os.path.join(input_directory, current_animal_id, 'Sequence_Automated', 'Session Data/')\n",
    "\n",
    "        # Load Behavioural data using the import_bpod_data_files function\n",
    "        behavior_data, total_sessions, path, session_dates = import_bpod_data_files(current_input_path)\n",
    "\n",
    "        # Initialize strings to store processed and skipped sessions\n",
    "        processed_sessions = ''\n",
    "        skipped_sessions = ''\n",
    "\n",
    "        # Iterate over each session\n",
    "        for session_index in range(total_sessions):\n",
    "\n",
    "            # Create unique identifier for the session\n",
    "            session_date = session_dates[session_index] + '_' + str(behavior_data[session_index]['__header__'])[-25:-22]\n",
    "\n",
    "            # Set the save path depending on the session number\n",
    "            if session_index < 10:\n",
    "                save_path = os.path.join(output_directory, current_animal_id, 'Preprocessed', f'0{session_index}_{session_date}')\n",
    "            else:\n",
    "                save_path = os.path.join(output_directory, current_animal_id, 'Preprocessed', f'{session_index}_{session_date}')\n",
    "\n",
    "            # Check if the directory exists already\n",
    "            if not os.path.isdir(save_path):\n",
    "                # If it doesn't exist, make the directory and set the processing flag to True\n",
    "                os.makedirs(save_path)\n",
    "                should_process = True\n",
    "            else:\n",
    "                # If it does exist, check the replace_existing flag to determine if data should be processed\n",
    "                should_process = replace_existing\n",
    "\n",
    "            # If processing flag is True, convert the data to a Python-friendly format\n",
    "            if should_process:\n",
    "                # Calculate final reward amount for the session\n",
    "                final_reward_amounts = []\n",
    "                for item in behavior_data[session_index]['SessionData']['SessionVariables']['TLevel']:\n",
    "                    training_level = item\n",
    "                    final_reward_amounts.append(behavior_data[0]['SessionData']['SessionVariables']['TrainingLevels'][training_level-1][4])\n",
    "\n",
    "            # fetch trial_settings \n",
    "            trial_settings = behavior_data[session_index]['SessionData']['TrialSettings'][0]\n",
    "\n",
    "            # Save out LED intensities and reward amounts on their own:\n",
    "            led_intensities = pd.DataFrame({\n",
    "                'Port2': behavior_data[session_index]['SessionData']['SessionVariables']['LEDIntensitys']['port2'],\n",
    "                'Port3': behavior_data[session_index]['SessionData']['SessionVariables']['LEDIntensitys']['port3'],\n",
    "                'Port4': behavior_data[session_index]['SessionData']['SessionVariables']['LEDIntensitys']['port4'],\n",
    "                'Port5': behavior_data[session_index]['SessionData']['SessionVariables']['LEDIntensitys']['port5']\n",
    "            })\n",
    "\n",
    "            # Create a DataFrame for reward amounts for each port:\n",
    "            reward_amounts = pd.DataFrame({\n",
    "                'Port1': behavior_data[session_index]['SessionData']['SessionVariables']['RewardAmount']['port1'],\n",
    "                'Port2': behavior_data[session_index]['SessionData']['SessionVariables']['RewardAmount']['port2'],\n",
    "                'Port3': behavior_data[session_index]['SessionData']['SessionVariables']['RewardAmount']['port3'],\n",
    "                'Port4': behavior_data[session_index]['SessionData']['SessionVariables']['RewardAmount']['port4']\n",
    "            })\n",
    "            \n",
    "            # TODO: Save out reward amounts?\n",
    "            # TODO: Save out LED intensities\n",
    "\n",
    "            # Extract PortIn times for each port and check for errors\n",
    "            port_in_times, port_out_times, port_references = extract_poke_times(behavior_data[session_index])\n",
    "\n",
    "            # Remove 'nan' values (these represent times when part of the event was dropped by Bpod for some reason)\n",
    "            fixed_port_in_times, fixed_port_out_times, fixed_port_references = remove_dropped_in_events(port_in_times, port_out_times, port_references)\n",
    "\n",
    "            # Resort these times for consistent chronology\n",
    "            sorted_port_in_times, sorted_port_out_times, sorted_port_references = sort_by_time(fixed_port_in_times, fixed_port_out_times, fixed_port_references)\n",
    "\n",
    "            # Extract reward timestamps:\n",
    "            reward_timestamps = extract_reward_timestamps(behavior_data[session_index])\n",
    "\n",
    "            # Find indices corresponding to rewarded events and align them to poke events:\n",
    "            rewarded_event_indices = find_rewarded_event_indices(sorted_port_in_times, sorted_port_references, reward_timestamps)\n",
    "\n",
    "            # Remove 'NaN' entries from reward timestamps:\n",
    "            reward_timestamps = np.asarray(reward_timestamps)\n",
    "            reward_timestamps = reward_timestamps[np.logical_not(np.isnan(reward_timestamps))]\n",
    "            reward_timestamps = list(reward_timestamps)\n",
    "\n",
    "            # Align reward timestamps to the corresponding poke events:\n",
    "            aligned_reward_timestamps = align_trigger_to_index(reward_timestamps, rewarded_event_indices, sorted_port_references)\n",
    "\n",
    "            # Extract trial start timestamps:\n",
    "            trial_start_timestamps = extract_trial_timestamps(behavior_data[session_index])\n",
    "            \n",
    "            # Extract trial end times:\n",
    "            trial_end_timestamps = extract_trial_end_times(behavior_data[session_index])\n",
    "\n",
    "            # Determine trial IDs:\n",
    "            trial_ids = determine_trial_id(sorted_port_in_times, trial_end_timestamps)\n",
    "\n",
    "            # Find trial start indices:\n",
    "            trial_start_indices = find_trial_start_indices(trial_ids)\n",
    "\n",
    "            # Align trial start timestamps to poke events:\n",
    "            aligned_trial_start_timestamps = align_trial_start_end_timestamps(trial_ids, trial_start_indices, trial_start_timestamps)\n",
    "\n",
    "            # Align trial end timestamps to poke events:\n",
    "            aligned_trial_end_timestamps = align_trial_start_end_timestamps(trial_ids, trial_start_indices, trial_end_timestamps)\n",
    "\n",
    "            # handle optogenetic stimulation\n",
    "            optotrials_aligned, optotrials_port_aligned = handle_opto_stim_data(behavior_data, trial_settings, session_index, trial_ids)\n",
    "\n",
    "            # Create empty lists to store intermediate rewards and LED intensities data for each trial\n",
    "            intermediate_rewards_data = []\n",
    "            led_intensities_data = []\n",
    "\n",
    "            # Iterate over 'TLevel' items in SessionVariables\n",
    "            for tlevel_item in behavior_data[session_index]['SessionData']['SessionVariables']['TLevel']:\n",
    "                tlevel = tlevel_item\n",
    "                # Append intermediate rewards and LED intensities data for the current trial\n",
    "                intermediate_rewards_data.append(\n",
    "                    list(behavior_data[session_index]['SessionData']['SessionVariables']['TrainingLevels'][tlevel-1][0:4])\n",
    "                )\n",
    "                led_intensities_data.append(\n",
    "                    list(behavior_data[session_index]['SessionData']['SessionVariables']['TrainingLevels'][tlevel-1][6:10])\n",
    "                )\n",
    "\n",
    "            # Align intermediate rewards and LED intensities data with trial start indices\n",
    "            aligned_led_intensities = align_trial_start_end_timestamps(trial_ids, trial_start_indices, led_intensities_data)\n",
    "            aligned_intermediate_rewards = align_trial_start_end_timestamps(trial_ids, trial_start_indices, intermediate_rewards_data)\n",
    "\n",
    "\n",
    "            # Align training level for each trial\n",
    "            training_levels = align_data_to_trial_ids(trial_ids, behavior_data[session_index]['SessionData']['SessionVariables']['TLevel'])\n",
    "\n",
    "           \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for: EJT244\n",
      "Processing data for: SP110\n",
      "Processing data for: SP111\n"
     ]
    }
   ],
   "source": [
    "process_animal_data(animal_ids=animal_ids, input_directory=input_directory, output_directory=output_directory, camera_directory=camera_directory, replace_existing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sthitapati/Documents/sequence_data/bpod_raw_data/SP111/Sequence_Automated/Session Data/\n"
     ]
    }
   ],
   "source": [
    "# Construct the path for the current animal's data\n",
    "current_animal_id = animal_ids[2]\n",
    "current_input_path = os.path.join(input_directory, current_animal_id, 'Sequence_Automated', 'Session Data/')\n",
    "\n",
    "print(current_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Behavioural data using the import_bpod_data_files function\n",
    "behavior_data, total_sessions, path, session_dates = import_bpod_data_files(current_input_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_index = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate final reward amount for the session\n",
    "final_reward_amounts = []\n",
    "for item in behavior_data[session_index]['SessionData']['SessionVariables']['TLevel']:\n",
    "    training_level = item\n",
    "    final_reward_amounts.append(behavior_data[session_index]['SessionData']['SessionVariables']['TrainingLevels'][training_level-1][4])\n",
    "\n",
    "# final_reward_amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch trial_settings \n",
    "trial_settings = behavior_data[session_index]['SessionData']['TrialSettings'][0]\n",
    "# trial_settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out LED intensities and reward amounts on their own:\n",
    "led_intensities = pd.DataFrame({\n",
    "    'Port2': behavior_data[session_index]['SessionData']['SessionVariables']['LEDIntensitys']['port2'],\n",
    "    'Port3': behavior_data[session_index]['SessionData']['SessionVariables']['LEDIntensitys']['port3'],\n",
    "    'Port4': behavior_data[session_index]['SessionData']['SessionVariables']['LEDIntensitys']['port4'],\n",
    "    'Port5': behavior_data[session_index]['SessionData']['SessionVariables']['LEDIntensitys']['port5']\n",
    "})\n",
    "# print(led_intensities)\n",
    "# Create a DataFrame for reward amounts for each port:\n",
    "reward_amounts = pd.DataFrame({\n",
    "    'Port1': behavior_data[session_index]['SessionData']['SessionVariables']['RewardAmount']['port1'],\n",
    "    'Port2': behavior_data[session_index]['SessionData']['SessionVariables']['RewardAmount']['port2'],\n",
    "    'Port3': behavior_data[session_index]['SessionData']['SessionVariables']['RewardAmount']['port3'],\n",
    "    'Port4': behavior_data[session_index]['SessionData']['SessionVariables']['RewardAmount']['port4']\n",
    "})\n",
    "# print(reward_amounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2870\n",
      "2870\n",
      "2870\n"
     ]
    }
   ],
   "source": [
    "# Extract PortIn times for each port and check for errors\n",
    "port_in_times, port_out_times, port_references = extract_poke_times(behavior_data[session_index])\n",
    "\n",
    "print(len(port_in_times))\n",
    "print(len(port_out_times))\n",
    "print(len(port_references))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2870\n",
      "2870\n",
      "2870\n"
     ]
    }
   ],
   "source": [
    "# Remove 'nan' values (these represent times when part of the event was dropped by Bpod for some reason)\n",
    "fixed_port_in_times, fixed_port_out_times, fixed_port_references = remove_dropped_in_events(port_in_times, port_out_times, port_references)\n",
    "\n",
    "print(len(fixed_port_in_times))\n",
    "print(len(fixed_port_out_times))\n",
    "print(len(fixed_port_references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2870\n",
      "2870\n",
      "2870\n"
     ]
    }
   ],
   "source": [
    "# Resort these times for consistent chronology\n",
    "sorted_port_in_times, sorted_port_out_times, sorted_port_references = sort_by_time(fixed_port_in_times, fixed_port_out_times, fixed_port_references)\n",
    "\n",
    "print(len(sorted_port_in_times))\n",
    "print(len(sorted_port_out_times))\n",
    "print(len(sorted_port_references))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract reward timestamps:\n",
    "reward_timestamps = extract_reward_timestamps(behavior_data[session_index])\n",
    "\n",
    "# Find indices corresponding to rewarded events and align them to poke events:\n",
    "rewarded_event_indices = find_rewarded_event_indices(sorted_port_in_times, sorted_port_references, reward_timestamps)\n",
    "\n",
    "# Remove 'NaN' entries from reward timestamps:\n",
    "reward_timestamps = np.asarray(reward_timestamps)\n",
    "reward_timestamps = reward_timestamps[np.logical_not(np.isnan(reward_timestamps))]\n",
    "reward_timestamps = list(reward_timestamps)\n",
    "\n",
    "# Align reward timestamps to the corresponding poke events:\n",
    "aligned_reward_timestamps = align_trigger_to_index(reward_timestamps, rewarded_event_indices, sorted_port_references)\n",
    "\n",
    "# print(len(aligned_reward_timestamps))\n",
    "# print(len(reward_timestamps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2870\n"
     ]
    }
   ],
   "source": [
    "# Align reward timestamps to the corresponding poke events:\n",
    "aligned_reward_timestamps = align_trigger_to_index(reward_timestamps, rewarded_event_indices, sorted_port_references)\n",
    "\n",
    "print(len(aligned_reward_timestamps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract trial start timestamps:\n",
    "trial_start_timestamps = extract_trial_timestamps(behavior_data[session_index])\n",
    "\n",
    "print(len(trial_start_timestamps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract trial end times:\n",
    "trial_end_timestamps = extract_trial_end_times(behavior_data[session_index])\n",
    "print(len(trial_end_timestamps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2870\n",
      "<class 'numpy.ndarray'>\n",
      "260\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(sorted_port_in_times))\n",
    "print(type(sorted_port_in_times))\n",
    "print(len(trial_end_timestamps))\n",
    "print(type(trial_end_timestamps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2870\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Determine trial IDs:\n",
    "trial_ids = determine_trial_id(sorted_port_in_times, trial_end_timestamps)\n",
    "print(len(trial_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find trial start indices:\n",
    "trial_start_indices = find_trial_start_indices(trial_ids)\n",
    "print(len(trial_start_indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2870\n",
      "2870\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Align trial start timestamps to poke events:\n",
    "aligned_trial_start_timestamps = align_trial_start_end_timestamps(trial_ids, trial_start_indices, trial_start_timestamps)\n",
    "print(len(aligned_trial_start_timestamps))\n",
    "\n",
    "# Align trial end timestamps to poke events:\n",
    "aligned_trial_end_timestamps = align_trial_start_end_timestamps(trial_ids, trial_start_indices, trial_end_timestamps)\n",
    "print(len(aligned_trial_end_timestamps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2870\n",
      "2870\n"
     ]
    }
   ],
   "source": [
    "# handle optogenetic stimulation\n",
    "optotrials_aligned, optotrials_port_aligned = handle_opto_stim_data(behavior_data, trial_settings, session_index, trial_ids)\n",
    "\n",
    "print(len(optotrials_aligned))\n",
    "print(len(optotrials_port_aligned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store intermediate rewards and LED intensities data for each trial\n",
    "intermediate_rewards_data = []\n",
    "led_intensities_data = []\n",
    "\n",
    "# Iterate over 'TLevel' items in SessionVariables\n",
    "for tlevel_item in behavior_data[session_index]['SessionData']['SessionVariables']['TLevel']:\n",
    "    tlevel = tlevel_item\n",
    "\n",
    "    # Append intermediate rewards and LED intensities data for the current trial\n",
    "    intermediate_rewards_data.append(\n",
    "        list(behavior_data[session_index]['SessionData']['SessionVariables']['TrainingLevels'][tlevel-1][0:4])\n",
    "    )\n",
    "    led_intensities_data.append(\n",
    "        list(behavior_data[session_index]['SessionData']['SessionVariables']['TrainingLevels'][tlevel-1][6:10])\n",
    "    )\n",
    "\n",
    "# Align intermediate rewards and LED intensities data with trial start indices\n",
    "aligned_led_intensities = align_trial_start_end_timestamps(trial_ids, trial_start_indices, led_intensities_data)\n",
    "aligned_intermediate_rewards = align_trial_start_end_timestamps(trial_ids, trial_start_indices, intermediate_rewards_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2870\n",
      "2870\n"
     ]
    }
   ],
   "source": [
    "print(len(aligned_led_intensities))\n",
    "print(len(aligned_intermediate_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2870\n"
     ]
    }
   ],
   "source": [
    "# Align training level for each trial\n",
    "training_levels = align_data_to_trial_ids(trial_ids, behavior_data[session_index]['SessionData']['SessionVariables']['TLevel'])\n",
    "print(len(training_levels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(session_dates[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date: 20230421_115225_Fri\n",
      "False\n",
      "None\n",
      "date: 20230422_152143_Sat\n",
      "False\n",
      "None\n",
      "date: 20230423_192319_Sun\n",
      "False\n",
      "None\n",
      "date: 20230424_135325_Mon\n",
      "False\n",
      "None\n",
      "date: 20230425_134730_Tue\n",
      "False\n",
      "None\n",
      "date: 20230426_113747_Wed\n",
      "False\n",
      "None\n",
      "date: 20230427_160502_Thu\n",
      "False\n",
      "None\n",
      "date: 20230428_120644_Fri\n",
      "False\n",
      "None\n",
      "date: 20230502_121059_Tue\n",
      "False\n",
      "None\n",
      "date: 20230503_121831_Wed\n",
      "False\n",
      "None\n",
      "date: 20230504_133913_Thu\n",
      "False\n",
      "None\n",
      "date: 20230509_115531_Tue\n",
      "False\n",
      "None\n",
      "date: 20230510_130339_Wed\n",
      "False\n",
      "None\n",
      "date: 20230511_142715_Thu\n",
      "False\n",
      "None\n",
      "date: 20230512_125459_Fri\n",
      "False\n",
      "None\n",
      "date: 20230516_155130_Tue\n",
      "False\n",
      "None\n",
      "date: 20230517_104733_Wed\n",
      "False\n",
      "None\n",
      "date: 20230518_171602_Thu\n",
      "False\n",
      "None\n",
      "date: 20230519_154240_Fri\n",
      "False\n",
      "None\n",
      "date: 20230523_181113_Tue\n",
      "False\n",
      "None\n",
      "date: 20230524_140934_Wed\n",
      "False\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for session_id in range(len(session_dates)):\n",
    "    filedate = session_dates[session_id] + '_' + str(behavior_data[session_id]['__header__'])[-25:-22]\n",
    "    print(f\"date: {filedate}\")\n",
    "\n",
    "    timestamps_exist, timestamp_file_path = find_camera_timestamps(session_date=session_dates[session_id], camera_directory=camera_directory, animal_id=current_animal_id)\n",
    "\n",
    "    print(timestamps_exist)\n",
    "    print(timestamp_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
