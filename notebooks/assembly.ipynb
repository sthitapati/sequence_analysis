{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "from typing import Dict, Tuple, List, Union, Optional\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to read the data from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadmat(filename: str) -> Dict:\n",
    "    '''\n",
    "    Load a .mat file and convert all mat-objects to nested dictionaries.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): The name of the .mat file to load.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the contents of the .mat file.\n",
    "    '''\n",
    "    # Load the .mat file\n",
    "    data = scipy.io.loadmat(filename, simplify_cells=True)\n",
    "    return data\n",
    "\n",
    "def import_bpod_data_files(input_path: str) -> Tuple[Dict[int, Dict], int, List[str], List[str]]:\n",
    "    '''\n",
    "    Load all '.mat' files in a given folder and convert them to Python format.\n",
    "\n",
    "    Parameters:\n",
    "    input_path (str): The path to the folder containing the '.mat' files.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[Dict[int, Dict], int, list, list]: A tuple containing the converted data, the number of sessions,\n",
    "    the list of file paths, and the list of file dates.\n",
    "    '''\n",
    "    # Get a list of all files in the input path\n",
    "    behav_path = sorted(os.listdir(input_path))\n",
    "    behav_data = {}  # Set up file dictionary\n",
    "    session_dates = []\n",
    "    sessions = 0  # For naming each data set within the main dictionary\n",
    "\n",
    "    # Loop through each file in the input path\n",
    "    for file in [f for f in behav_path if f.endswith('.mat') and os.stat(input_path + f).st_size > 200000]:\n",
    "        # Check if the file is not the weird hidden file\n",
    "        if file != '.DS_Store':\n",
    "            # Load the '.mat' file and add it to the dictionary\n",
    "            current_file = loadmat(input_path + file)\n",
    "            behav_data[sessions] = current_file\n",
    "            sessions += 1\n",
    "            session_dates.append(file[-19:-4])\n",
    "\n",
    "    return behav_data, sessions, behav_path, session_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to various data transformations and wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start writing functions here one by one and then test them in this notebook\n",
    "\n",
    "def extract_poke_times(behavior_data: Dict) -> Tuple[List, List, List]:\n",
    "    \"\"\"\n",
    "    Extracts all port in/out times across the session for each port. \n",
    "    It aligns them to trial start timestamps so that the port in times \n",
    "    are across the whole session.\n",
    "\n",
    "    Parameters:\n",
    "    behavior_data (dict): The dictionary containing behavior data for the session.\n",
    "\n",
    "    Returns:\n",
    "    Tuple: Lists of all port in times, port out times, and corresponding port references.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store port in times, port out times and corresponding port references\n",
    "    all_port_in_times = []\n",
    "    all_port_out_times = []\n",
    "    all_port_references = []\n",
    "\n",
    "    # Iterate over each port\n",
    "    for port in range(1, 9):\n",
    "\n",
    "        # Initialize lists to store port in/out times for each port\n",
    "        port_in_times = []\n",
    "        port_out_times = []\n",
    "\n",
    "        # Iterate over each trial\n",
    "        for trial_index in range(behavior_data['SessionData']['nTrials']):\n",
    "\n",
    "            # Extract port in times\n",
    "            if f'Port{port}In' in behavior_data['SessionData']['RawEvents']['Trial'][trial_index]['Events']:\n",
    "                trial_start_timestamp = behavior_data['SessionData']['TrialStartTimestamp'][trial_index]\n",
    "                port_in_ts_offset = behavior_data['SessionData']['RawEvents']['Trial'][trial_index]['Events'][f'Port{port}In']\n",
    "                port_in_ts = trial_start_timestamp + port_in_ts_offset\n",
    "\n",
    "                # If port in timestamp is a single value, convert it to a list\n",
    "                if isinstance(port_in_ts, np.float64):\n",
    "                    port_in_ts = [port_in_ts]\n",
    "\n",
    "                # Add port in times to the list\n",
    "                port_in_times.extend(port_in_ts)\n",
    "\n",
    "            # Extract port out times\n",
    "            if f'Port{port}Out' in behavior_data['SessionData']['RawEvents']['Trial'][trial_index]['Events']:\n",
    "                trial_start_timestamp = behavior_data['SessionData']['TrialStartTimestamp'][trial_index]\n",
    "                port_out_ts_offset = behavior_data['SessionData']['RawEvents']['Trial'][trial_index]['Events'][f'Port{port}Out']\n",
    "                port_out_ts = trial_start_timestamp + port_out_ts_offset\n",
    "\n",
    "                # If port out timestamp is a single value, convert it to a list\n",
    "                if isinstance(port_out_ts, np.float64):\n",
    "                    port_out_ts = [port_out_ts]\n",
    "\n",
    "                # Add port out times to the list\n",
    "                port_out_times.extend(port_out_ts)\n",
    "\n",
    "        # Check if the number of port in times and port out times are equal\n",
    "        # If not, apply error check and fix\n",
    "        if len(port_in_times) != len(port_out_times):\n",
    "            port_in_times, port_out_times = error_check_and_fix(port_in_times, port_out_times)\n",
    "\n",
    "        # Add port in times, port out times and port references to the overall lists\n",
    "        all_port_references.extend([port] * len(port_in_times))\n",
    "        all_port_in_times.extend(port_in_times)\n",
    "        all_port_out_times.extend(port_out_times)\n",
    "\n",
    "    return all_port_in_times, all_port_out_times, all_port_references\n",
    "\n",
    "def error_check_and_fix(port_in_times: List, port_out_times: List) -> Tuple[List, List]:\n",
    "    \"\"\"\n",
    "    Checks and corrects mismatches in the length of port in and port out times lists.\n",
    "    If lengths are unequal, 'nan' is inserted at the appropriate position or appended to the shorter list.\n",
    "\n",
    "    Parameters:\n",
    "    port_in_times (List): The list of port in times.\n",
    "    port_out_times (List): The list of port out times.\n",
    "\n",
    "    Returns:\n",
    "    Tuple: The corrected port in times and port out times lists.\n",
    "    \"\"\"\n",
    "    # Initialize fixed flag as False\n",
    "    fixed = False\n",
    "\n",
    "    # If the lengths of port in times and port out times lists are not equal\n",
    "    if len(port_in_times) != len(port_out_times):\n",
    "\n",
    "        # If port in times list is longer than port out times list\n",
    "        if len(port_in_times) > len(port_out_times):\n",
    "            # Iterate over each item in the port out times list\n",
    "            for i in range(len(port_out_times)):\n",
    "                # If the port out time is later than the next port in time\n",
    "                if port_out_times[i] >= port_in_times[i+1]:\n",
    "                    # Insert a 'nan' at this position in the port out times list\n",
    "                    port_out_times.insert(i, 'nan')\n",
    "                    fixed = True\n",
    "\n",
    "            # If the issue wasn't fixed by the above process, append 'nan' to port out times list\n",
    "            if len(port_in_times) > len(port_out_times) and not fixed:\n",
    "                port_out_times.append('nan')\n",
    "\n",
    "        # If port out times list is longer than port in times list\n",
    "        elif len(port_out_times) > len(port_in_times):\n",
    "            # Iterate over each item in the port in times list\n",
    "            for i in range(len(port_in_times)):\n",
    "                # If the port in time is later than or equal to the port out time\n",
    "                if port_in_times[i] >= port_out_times[i]:\n",
    "                    # Insert a 'nan' at this position in the port in times list\n",
    "                    port_in_times.insert(i, 'nan')\n",
    "                    fixed = True\n",
    "\n",
    "            # If the issue wasn't fixed by the above process, append 'nan' to port in times list\n",
    "            if len(port_out_times) > len(port_in_times) and not fixed:\n",
    "                port_in_times.append('nan')\n",
    "\n",
    "    # If the lengths of port in times and port out times lists are still not equal\n",
    "    if len(port_in_times) != len(port_out_times):\n",
    "        print('Dropped event not fixed!!!!')\n",
    "\n",
    "    return port_in_times, port_out_times\n",
    "\n",
    "def remove_dropped_in_events(port_in_times: List, port_out_times: List, port_references: List) -> Tuple[List, List, List]:\n",
    "    \"\"\"\n",
    "    Cleans up the data by removing 'nan' values from the lists of port in times, port out times, and port references.\n",
    "\n",
    "    Parameters:\n",
    "    port_in_times (List): The list of port in times.\n",
    "    port_out_times (List): The list of port out times.\n",
    "    port_references (List): The list of port references.\n",
    "\n",
    "    Returns:\n",
    "    Tuple: The cleaned port in times, port out times, and port references lists.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a reversed list of indexes to remove in order to avoid index shifting during removal\n",
    "    indexes_to_remove = [i for i, time in enumerate(port_in_times) if time == 'nan'][::-1]\n",
    "\n",
    "    for index in indexes_to_remove:\n",
    "        # Remove 'nan' entries from each list\n",
    "        del port_in_times[index]\n",
    "        del port_out_times[index]\n",
    "        del port_references[index]\n",
    "\n",
    "    return port_out_times, port_in_times, port_references\n",
    "\n",
    "def sort_by_time(port_in_times: List, port_out_times: List, port_references: List) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Sorts the data by port in times. If an out time is missing, it will be appended with 'nan'.\n",
    "\n",
    "    Parameters:\n",
    "    port_in_times (List): The list of port in times.\n",
    "    port_out_times (List): The list of port out times.\n",
    "    port_references (List): The list of port references.\n",
    "\n",
    "    Returns:\n",
    "    Tuple: The sorted port in times, port out times, and port references.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the indices that would sort the in times\n",
    "    sort_indices = np.argsort(port_in_times)\n",
    "\n",
    "    # Apply the sorted indices to each list and convert them to numpy arrays\n",
    "    sorted_in_times = np.array(port_in_times, dtype=float)[sort_indices]\n",
    "    sorted_references = np.array(port_references)[sort_indices]\n",
    "    \n",
    "    # Check if the number of out times matches the number of sorted indices\n",
    "    if len(sort_indices) == len(port_out_times):\n",
    "        sorted_out_times = np.array(port_out_times, dtype=float)[sort_indices]\n",
    "    else:\n",
    "        # If they don't match, append a 'nan' to the out times before sorting\n",
    "        sorted_out_times = np.array(port_out_times + [np.nan], dtype=float)[sort_indices]\n",
    "\n",
    "    return sorted_in_times, sorted_out_times, sorted_references\n",
    "\n",
    "def extract_reward_timestamps(behavior_data: Dict) -> List[float]:\n",
    "    '''\n",
    "    Extracts all reward timestamps across a session for each port.\n",
    "\n",
    "    Parameters:\n",
    "    behavior_data (Dict): The behavioral data dictionary.\n",
    "\n",
    "    Returns:\n",
    "    List[float]: A list containing all the reward timestamps for the session.\n",
    "    '''\n",
    "    # Initialize list to store all reward timestamps\n",
    "    reward_timestamps = []\n",
    "\n",
    "    # Iterate over each trial in the session\n",
    "    for trial in range(behavior_data['SessionData']['nTrials']):\n",
    "        \n",
    "        # Check if the 'Reward' event exists in the trial data\n",
    "        if 'Reward' in behavior_data['SessionData']['RawEvents']['Trial'][trial]['States']:\n",
    "            \n",
    "            # Calculate the timestamp of the reward event relative to the start of the trial\n",
    "            trial_start_timestamp = behavior_data['SessionData']['TrialStartTimestamp'][trial]\n",
    "            reward_time_offset = behavior_data['SessionData']['RawEvents']['Trial'][trial]['States']['Reward'][0]\n",
    "            \n",
    "            # Convert the reward timestamp to the session timeline\n",
    "            reward_timestamp = trial_start_timestamp + reward_time_offset\n",
    "            \n",
    "            # Add the reward timestamp to the list of reward timestamps\n",
    "            reward_timestamps.append(reward_timestamp)\n",
    "\n",
    "    return reward_timestamps\n",
    "\n",
    "def find_rewarded_event_indices(sorted_in_timestamps: List[float], \n",
    "                                sorted_port_references: List[int], \n",
    "                                reward_timestamps: List[float]) -> List[int]:\n",
    "    '''\n",
    "    Identifies the indices of rewarded events.\n",
    "\n",
    "    Parameters:\n",
    "    sorted_in_timestamps (List[float]): List of sorted poke in timestamps.\n",
    "    sorted_port_references (List[int]): List of port references corresponding to the poke in timestamps.\n",
    "    reward_timestamps (List[float]): List of reward timestamps.\n",
    "\n",
    "    Returns:\n",
    "    List[int]: Indices of rewarded events in sorted_in_timestamps and sorted_port_references.\n",
    "    '''\n",
    "\n",
    "    rewarded_event_indices = []  # Initialize the list to store indices of rewarded events\n",
    "    reward_index = 0  # Initialize reward index counter\n",
    "\n",
    "    # Iterate over sorted port references with their indices\n",
    "    for event_index, port_number in enumerate(sorted_port_references):\n",
    "        \n",
    "        # Check if port number is 7 and there are reward timestamps left to process\n",
    "        if port_number == 7 and reward_index < len(reward_timestamps):\n",
    "            \n",
    "            # Skip NaN timestamps\n",
    "            while np.isnan(reward_timestamps[reward_index]):\n",
    "                reward_index += 1\n",
    "\n",
    "                # If there are no more reward timestamps, exit the loop\n",
    "                if reward_index >= len(reward_timestamps):\n",
    "                    break\n",
    "\n",
    "            # If there are still reward timestamps left, check if the in time is greater than or equal to the current reward timestamp\n",
    "            if reward_index < len(reward_timestamps) and sorted_in_timestamps[event_index] >= reward_timestamps[reward_index]:\n",
    "                \n",
    "                # If so, record the event index as a rewarded event\n",
    "                rewarded_event_indices.append(event_index)\n",
    "                \n",
    "                # And move on to the next reward timestamp\n",
    "                reward_index += 1\n",
    "\n",
    "    return rewarded_event_indices\n",
    "\n",
    "def align_trigger_to_index(triggers: List[float], \n",
    "                           trigger_indices: List[int], \n",
    "                           all_timestamps: List[float]) -> List[Union[float, str]]:\n",
    "    '''\n",
    "    Aligns triggers to their corresponding indices in the timestamp array.\n",
    "\n",
    "    Parameters:\n",
    "    triggers (List[float]): List of trigger timestamps.\n",
    "    trigger_indices (List[int]): List of indices corresponding to trigger timestamps.\n",
    "    all_timestamps (List[float]): List of all timestamps.\n",
    "\n",
    "    Returns:\n",
    "    List[Union[float, str]]: Array with triggers aligned to their indices, \n",
    "                              and 'NaN' for all other indices.\n",
    "    '''\n",
    "\n",
    "    # Initialize output array with 'NaN' for all indices\n",
    "    aligned_triggers = ['NaN'] * len(all_timestamps)\n",
    "    \n",
    "    # Assign trigger values to their corresponding indices\n",
    "    for trigger_value, trigger_index in zip(triggers, trigger_indices):\n",
    "        aligned_triggers[trigger_index] = trigger_value\n",
    "\n",
    "    return aligned_triggers\n",
    "\n",
    "def extract_trial_timestamps(behavior_data):\n",
    "    \"\"\"\n",
    "    Extracts trial timestamps from behavioral data.\n",
    "\n",
    "    Args:\n",
    "        behavior_data: The complete behavioral data dictionary.\n",
    "\n",
    "    Returns:\n",
    "        A list of trial timestamps.\n",
    "    \"\"\"\n",
    "    trial_timestamps = []\n",
    "    for trial in range(behavior_data['SessionData']['nTrials']):\n",
    "        trial_start_timestamp = behavior_data['SessionData']['TrialStartTimestamp'][trial]\n",
    "        trial_timestamps.append(trial_start_timestamp)\n",
    "    return trial_timestamps\n",
    "\n",
    "def extract_trial_end_times(behavior_data):\n",
    "    \"\"\"\n",
    "    Extracts trial end times from behavioral data.\n",
    "\n",
    "    Args:\n",
    "        behavior_data: The complete behavioral data dictionary.\n",
    "\n",
    "    Returns:\n",
    "        A list of trial end times.\n",
    "    \"\"\"\n",
    "\n",
    "    all_end_times = []\n",
    "    for trial in range(behavior_data['SessionData']['nTrials']):\n",
    "        if 'ExitSeq' in behavior_data['SessionData']['RawEvents']['Trial'][trial]['States']:\n",
    "            trial_start_timestamp = behavior_data['SessionData']['TrialStartTimestamp'][trial]\n",
    "            exit_time_offset = behavior_data['SessionData']['RawEvents']['Trial'][trial]['States']['ExitSeq'][-1]\n",
    "            end_times = trial_start_timestamp + exit_time_offset\n",
    "            all_end_times.append(end_times)\n",
    "    return all_end_times\n",
    "\n",
    "def determine_trial_id(sorted_port_in_times, trial_end_timestamps):\n",
    "    \"\"\"\n",
    "    Determines the trial id for each port event.\n",
    "\n",
    "    Args:\n",
    "        sorted_port_in_times: Sorted list of port in times.\n",
    "        trial_end_timestamps: List of trial end times.\n",
    "\n",
    "    Returns:\n",
    "        A list of trial ids for each port event.\n",
    "    \"\"\"\n",
    "\n",
    "    trial_id = []\n",
    "    trial_number = 1\n",
    "    for time in sorted_port_in_times:\n",
    "        if trial_number > len(trial_end_timestamps):\n",
    "            trial_id.append(trial_number)\n",
    "            continue\n",
    "        if float(time) <= trial_end_timestamps[trial_number - 1]:\n",
    "            trial_id.append(trial_number)\n",
    "        else:\n",
    "            trial_number += 1\n",
    "            trial_id.append(trial_number)\n",
    "    return trial_id\n",
    "\n",
    "def find_trial_start_indices(trial_ids):\n",
    "    \"\"\"\n",
    "    Determines the start indices for each trial.\n",
    "\n",
    "    Args:\n",
    "        trial_ids: List of trial ids for each port event.\n",
    "\n",
    "    Returns:\n",
    "        A list of start indices for each trial.\n",
    "    \"\"\"\n",
    "\n",
    "    trial_start_indices = [0]\n",
    "    for index, trial_id in enumerate(trial_ids[1:], 1):  # start enumerating from 1\n",
    "        if trial_id != trial_ids[index-1]:\n",
    "            trial_start_indices.append(index)\n",
    "    return trial_start_indices\n",
    "\n",
    "def align_trial_start_end_timestamps(trial_ids, trial_start_indices, trial_start_timestamps):\n",
    "    \"\"\"\n",
    "    Aligns trial start and end timestamps.\n",
    "\n",
    "    Args:\n",
    "        trial_ids: List of trial ids for each port event.\n",
    "        trial_start_indices: List of start indices for each trial.\n",
    "        trial_start_timestamps: List of trial start times.\n",
    "\n",
    "    Returns:\n",
    "        A list of aligned trial start times.\n",
    "    \"\"\"\n",
    "\n",
    "    aligned_trial_timestamps = []\n",
    "    counter = 0\n",
    "    for i in range(len(trial_ids)):\n",
    "        if counter + 1 < len(trial_start_indices) and i == trial_start_indices[counter+1]:\n",
    "            counter += 1\n",
    "        if counter < len(trial_start_timestamps):\n",
    "            aligned_trial_timestamps.append(trial_start_timestamps[counter])\n",
    "        else:\n",
    "            aligned_trial_timestamps.append(np.nan)\n",
    "\n",
    "    if len(trial_start_timestamps) != len(trial_start_indices):\n",
    "        difference = abs(len(trial_start_timestamps) - len(trial_start_indices))\n",
    "        if difference > 2:\n",
    "            warnings.warn(f\"Difference between trial_start_timestamps and trial_start_indices exceeds 2: {difference}\")\n",
    "\n",
    "    return aligned_trial_timestamps\n",
    "\n",
    "def align_data_to_trial_ids(trial_ids: List[int], data: List[int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    This function aligns the given data according to the trial ids.\n",
    "\n",
    "    Args:\n",
    "        trial_ids (List[int]): The list of trial ids.\n",
    "        data (List[int]): The list of data to align.\n",
    "\n",
    "    Returns:\n",
    "        List[int]: The list of aligned data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the counter for executed trials and list for aligned trials\n",
    "    data_counter = 0\n",
    "    aligned_data = []\n",
    "\n",
    "    # Iterate over the list of trial ids\n",
    "    for index, trial_id in enumerate(trial_ids):\n",
    "        # For the first trial, simply append the first data item\n",
    "        if index == 0:\n",
    "            aligned_data.append(data[data_counter])\n",
    "        else:\n",
    "            # If the current trial id is same as previous one, append the same data item\n",
    "            if trial_id == trial_ids[index-1]:\n",
    "                if data_counter < len(data):\n",
    "                    aligned_data.append(data[data_counter])\n",
    "                else:\n",
    "                    aligned_data.append(float('nan'))\n",
    "            else:\n",
    "                # If the trial id has changed, increment the counter\n",
    "                data_counter += 1\n",
    "                # Check if data_counter has not exceeded the length of data\n",
    "                if data_counter < len(data):\n",
    "                    # Append the next data item\n",
    "                    aligned_data.append(data[data_counter])\n",
    "                else:\n",
    "                    # If data_counter has exceeded the length of data, append NaN or any other suitable value\n",
    "                    aligned_data.append(float('nan'))\n",
    "\n",
    "    return aligned_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "### handle data for optogenetics experiments ###\n",
    "### ---------------------------------------- ###\n",
    "\n",
    "def handle_opto_stim_data(behavior_data, trial_settings, session_index, trial_ids):\n",
    "    \"\"\"\n",
    "    Handles the optostim data. If optostim was enabled, creates a dataframe of optostim settings and\n",
    "    aligns optostim trial data to the trial data. If optostim was not enabled, creates a list of 'NaN' values.\n",
    "    If StimPoke was set to 5, includes additional variables in the settings dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    behavior_data (dict): The behavior data dictionary.\n",
    "    trial_settings (dict): The trial settings dictionary.\n",
    "    session_index (int): The current session index.\n",
    "    trial_ids (list): List of trial ids.\n",
    "\n",
    "    Returns:\n",
    "    optotrials_aligned (list): The list of aligned optostim trial data.\n",
    "    optotrials_port_aligned (list): The list of aligned optostim port data.\n",
    "    \"\"\"\n",
    "    if trial_settings['GUI']['OptoStim'] == 1:\n",
    "        # Create opto settings as a dataframe\n",
    "        opto_settings = pd.DataFrame({\n",
    "            'StimPoke': [trial_settings['GUI']['StimPoke']],\n",
    "            'PulsePower': [trial_settings['GUI']['PulsePower']],\n",
    "            'OptoChance': [trial_settings['GUI']['OptoChance']],\n",
    "            'PulseDuration': [trial_settings['GUI']['PulseDuration']],\n",
    "            'PulseInterval': [trial_settings['GUI']['PulseInterval']],\n",
    "            'TrainDuration': [trial_settings['GUI']['TrainDuration']],\n",
    "            'TrainDelay': [trial_settings['GUI']['TrainDelay']] if 'TrainDelay' in trial_settings['GUI'] else [None]\n",
    "        })\n",
    "\n",
    "        # Pull out optotrials from data if available\n",
    "        optotrials = behavior_data[session_index]['SessionData']['SessionVariables']['OptoStim']\n",
    "\n",
    "        # Align these to dataframe\n",
    "        executed_optotrials = optotrials[0:trial_ids[-1]]\n",
    "        optotrials_aligned = align_data_to_trial_ids(trial_ids, executed_optotrials)\n",
    "\n",
    "        # Determine stimulated port\n",
    "        if trial_settings['GUI']['StimPoke'] == 5:\n",
    "            port_stimulated_data = behavior_data[session_index]['SessionData']['SessionVariables']['PortStimulated']\n",
    "            optotrials_port = []\n",
    "            for i in range(len(trial_ids)):\n",
    "                if executed_optotrials[i] == 0:  # If no optostim, insert NaN\n",
    "                    optotrials_port.append(float('nan'))\n",
    "                else:\n",
    "                    optotrials_port.append(np.where(port_stimulated_data[i] == 1)[0][0] + 1)  # Adding 1 to match port numbers 1 through 4\n",
    "\n",
    "        else:\n",
    "            optotrials_port = []\n",
    "            for i, trial_id in enumerate(trial_ids):\n",
    "                if executed_optotrials[i] == 0:  # If no optostim, insert NaN\n",
    "                    optotrials_port.append(float('nan'))\n",
    "                else:\n",
    "                    optotrials_port.append(trial_settings['GUI']['StimPoke'])\n",
    "\n",
    "        # align ports to dataframe\n",
    "        optotrials_port_aligned = align_data_to_trial_ids(trial_ids, optotrials_port)\n",
    "    else:\n",
    "        # No optostim so fill this column with NaNs\n",
    "        optotrials_aligned = ['NaN'] * len(trial_ids)\n",
    "        optotrials_port_aligned = ['NaN'] * len(trial_ids)\n",
    "     \n",
    "    return optotrials_aligned, optotrials_port_aligned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "### These functions are used to handle the data for the camera timestamps ###\n",
    "### --------------------------------------------------------------------- ###\n",
    "\n",
    "def handle_camera_data(session_date, camera_directory, current_animal_id, trial_ids, trial_start_indices, sorted_port_references, save_path):\n",
    "    \"\"\"\n",
    "    Handle the processing of camera timestamps for a specific animal and session.\n",
    "\n",
    "    Args:\n",
    "        session_date (str): Session date.\n",
    "        camera_directory (str): Directory path for camera data.\n",
    "        current_animal_id (str): ID of the animal being processed.\n",
    "        trial_ids (list): List of trial IDs.\n",
    "        trial_start_indices (list): List of trial start indices.\n",
    "        sorted_port_references (list): List of sorted port references.\n",
    "        save_path (str): Path to save the preprocessed camera data.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of aligned start, end, and first poke trial timestamps.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine if camera timestamps exist for the session\n",
    "    do_timestamps_exist, timestamp_file_path = find_camera_timestamps(session_date, camera_directory, current_animal_id)\n",
    "\n",
    "    # Initialize the arrays with 'NaN'\n",
    "    aligned_start_trial_timestamps = ['NaN'] * len(trial_ids)\n",
    "    aligned_end_trial_timestamps = ['NaN'] * len(trial_ids)\n",
    "    aligned_first_poke_timestamps = ['NaN'] * len(trial_ids)\n",
    "\n",
    "    if do_timestamps_exist:\n",
    "        print('Timestamps found for session.')\n",
    "        \n",
    "        # Load camera timestamps\n",
    "        raw_camera_timestamps_df = load_camera_timestamps_from_file(input_file_path=timestamp_file_path)\n",
    "        \n",
    "        # Convert to seconds and uncycle timestamps\n",
    "        camera_timestamps = convert_and_uncycle_timestamps(camera_timestamps_df=raw_camera_timestamps_df)\n",
    "        \n",
    "        # Check for dropped frames\n",
    "        check_for_dropped_frames(timestamps=camera_timestamps, expected_frame_rate=60)\n",
    "        \n",
    "        # Find trigger states\n",
    "        camera_trigger_states = determine_trigger_states_from_raw_timestamps(raw_camera_timestamps_df=raw_camera_timestamps_df)\n",
    "        \n",
    "        # Check if triggers are working\n",
    "        are_triggers_broken = np.max(camera_trigger_states) == np.min(camera_trigger_states)\n",
    "        \n",
    "        if not are_triggers_broken:\n",
    "            # Construct camera dataframe\n",
    "            camera_dataframe = pd.DataFrame(\n",
    "                {\n",
    "                    'timestamps': camera_timestamps,\n",
    "                    'trigger_states': camera_trigger_states,\n",
    "                    'datapath': [timestamp_file_path] * len(camera_timestamps)\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Save the dataframe\n",
    "            camera_dataframe.to_csv(os.path.join(save_path, 'preprocessed_cameradata.csv'))\n",
    "\n",
    "            # Find camera indices for trial start and first poke\n",
    "            trial_start_camera_indices, first_poke_indices = find_trial_start_and_poke1_camera_indices(camera_trigger_states=camera_trigger_states)\n",
    "            \n",
    "            # Align behavioural data (trial starts) with camera timestamps\n",
    "            aligned_start_trial_timestamps = align_trial_start_end_timestamps(\n",
    "                trial_ids=trial_ids,\n",
    "                trial_start_indices=trial_start_indices,\n",
    "                camera_timestamps=camera_timestamps[trial_start_indices]\n",
    "            )\n",
    "            \n",
    "            # Align behavioural data (trial ends) with camera timestamps\n",
    "            aligned_end_trial_timestamps = generate_aligned_trial_end_camera_timestamps(\n",
    "                trial_start_camera_indices=trial_start_camera_indices,\n",
    "                trial_ids=trial_ids,\n",
    "                camera_timestamps=camera_timestamps\n",
    "            )\n",
    "            \n",
    "            # Align behavioural data (first poke in port1) with camera timestamps\n",
    "            aligned_first_poke_timestamps = align_firstpoke_camera_timestamps(\n",
    "                trial_ids=trial_ids,\n",
    "                trial_start_indices=trial_start_indices,\n",
    "                trial_start_timestamps=camera_timestamps[first_poke_indices],\n",
    "                all_port_references_sorted=sorted_port_references,\n",
    "            )\n",
    "        else:\n",
    "            print('Camera trigger malfunction detected in session.')\n",
    "    else:\n",
    "        print('No camera timestamps found for the given session.')\n",
    "    \n",
    "    return do_timestamps_exist, aligned_start_trial_timestamps, aligned_end_trial_timestamps, aligned_first_poke_timestamps\n",
    "\n",
    "\n",
    "def find_trial_start_and_poke1_camera_indices(camera_trigger_states: np.ndarray) -> Tuple[List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Find indices in the camera timestamps where the trial starts and the first poke happens.\n",
    "\n",
    "    Args:\n",
    "        camera_trigger_states (np.ndarray): Array of trigger states from the camera.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[int], List[int]]: Lists of indices where trial starts and the first poke happens.\n",
    "    \"\"\"\n",
    "    ttl_change_indices = list(np.where(np.roll(camera_trigger_states, 1) != camera_trigger_states)[0])\n",
    "    if ttl_change_indices[0] == 0:\n",
    "        ttl_change_indices = ttl_change_indices[1:]\n",
    "\n",
    "    poke1_camera_indices = ttl_change_indices[1::2]\n",
    "    trial_start_camera_indices = ttl_change_indices[0::2]\n",
    "\n",
    "    return trial_start_camera_indices, poke1_camera_indices\n",
    "\n",
    "\n",
    "def generate_aligned_trial_end_camera_timestamps(trial_start_camera_indices: List[int], trial_ids: List[int], trial_start_indices: List[int], camera_timestamps: np.ndarray) -> List[Union[float, str]]:\n",
    "    \"\"\"\n",
    "    Generate aligned timestamps for the end of trials based on camera timestamps.\n",
    "\n",
    "    Args:\n",
    "        trial_start_camera_indices (List[int]): List of indices where each trial starts.\n",
    "        trial_ids (List[int]): List of trial ids for each port event.\n",
    "        trial_start_indices (List[int]): List of start indices for each trial.\n",
    "        camera_timestamps (np.ndarray): Array of camera timestamps.\n",
    "\n",
    "    Returns:\n",
    "        List[Union[float, str]]: List of aligned trial end timestamps.\n",
    "    \"\"\"\n",
    "    end_indices = [item for index, item in enumerate(trial_start_camera_indices) if index > 0]\n",
    "    aligned_trial_end_timestamps = align_trial_start_end_timestamps(trial_ids, trial_start_indices, camera_timestamps[end_indices])\n",
    "\n",
    "    last_trial_length = len(trial_ids) - trial_start_indices[-1]\n",
    "    if len(aligned_trial_end_timestamps) == len(trial_ids):\n",
    "        del aligned_trial_end_timestamps[-last_trial_length:]\n",
    "\n",
    "    aligned_trial_end_timestamps += ['NaN'] * last_trial_length\n",
    "    return aligned_trial_end_timestamps\n",
    "\n",
    "\n",
    "def align_firstpoke_camera_timestamps(trial_ids: List[int], trial_start_indices: List[int], trial_start_timestamps: List[float], all_port_references_sorted: List[float]) -> List[Union[float, str]]:\n",
    "    \"\"\"\n",
    "    Align the timestamps of the first poke with the camera timestamps.\n",
    "\n",
    "    Args:\n",
    "        trial_ids (List[int]): List of trial ids for each port event.\n",
    "        trial_start_indices (List[int]): List of start indices for each trial.\n",
    "        trial_start_timestamps (List[float]): List of trial start timestamps.\n",
    "        all_port_references_sorted (List[float]): Sorted list of all port references.\n",
    "\n",
    "    Returns:\n",
    "        List[Union[float, str]]: List of aligned first poke timestamps.\n",
    "    \"\"\"\n",
    "    trial_timestamps_aligned = []\n",
    "    counter = 0\n",
    "    for index, item in enumerate(trial_ids):\n",
    "        if all_port_references_sorted[index] == 2.0:\n",
    "            if item > counter:\n",
    "                counter += 1\n",
    "                if len(trial_start_timestamps) != counter - 1:\n",
    "                    trial_timestamps_aligned.append(trial_start_timestamps[counter-1])\n",
    "                else:\n",
    "                    trial_timestamps_aligned.append('NaN')\n",
    "            else:\n",
    "                trial_timestamps_aligned.append('NaN')\n",
    "        else:\n",
    "            trial_timestamps_aligned.append('NaN')\n",
    "    return trial_timestamps_aligned\n",
    "\n",
    "def find_camera_timestamps(session_date: str, camera_directory: str, animal_id: str) -> Tuple[bool, Union[str, None]]:\n",
    "    \"\"\"\n",
    "    Searches for timestamp files for a given animal and session date in the camera directory.\n",
    "    \n",
    "    Args:\n",
    "        session_date (str): The date of the session, in 'yyyymmddHHMMSS' format.\n",
    "        camera_directory (str): The path to the directory where camera files are stored.\n",
    "        animal_id (str): The ID of the animal.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[bool, Union[str, None]]: A tuple with a boolean indicating whether the timestamp file exists,\n",
    "        and the path to the timestamp file, if it exists. If no timestamp file is found, the path is None.\n",
    "    \"\"\"\n",
    "    # Format the session date in 'ddmmyy' format\n",
    "    formatted_date = session_date[6:8] + session_date[4:6] + session_date[2:4]\n",
    "\n",
    "    timestamps_exist = False\n",
    "    timestamp_file_path = None\n",
    "\n",
    "    # Check if the camera directory for the animal exists\n",
    "    animal_camera_directory = os.path.join(camera_directory, animal_id)\n",
    "    if not os.path.isdir(animal_camera_directory):\n",
    "        return timestamps_exist, timestamp_file_path\n",
    "\n",
    "    # Check if there is a directory for the session date\n",
    "    if formatted_date in os.listdir(animal_camera_directory):\n",
    "        session_date_directory = os.path.join(animal_camera_directory, formatted_date)\n",
    "\n",
    "        # Look for timestamp file in the session date directory\n",
    "        for filename in os.listdir(session_date_directory):\n",
    "            # Check if the file is a csv file\n",
    "            if filename.endswith('.csv'):\n",
    "                # Extract timestamp from filename\n",
    "                file_timestamp = filename[-12:-4].replace(\"_\", \"\")\n",
    "\n",
    "                # Check if the file was created before the session start time\n",
    "                if int(file_timestamp) < int(session_date[9:15]):\n",
    "                    timestamps_exist = True\n",
    "                    timestamp_file_path = os.path.join(session_date_directory, filename)\n",
    "                    break\n",
    "\n",
    "    return timestamps_exist, timestamp_file_path\n",
    "\n",
    "\n",
    "### Timestamp preprocessing:\n",
    "\n",
    "def load_camera_timestamps_from_file(input_file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads camera timestamps from a file and returns them as a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        input_file_path (str): The path of the file containing camera timestamps.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe containing camera timestamps.\n",
    "    \"\"\"\n",
    "    camera_timestamps_df = pd.read_csv(input_file_path, sep=' ', header=None, names=['Trigger', 'Timestamp', 'blank'], index_col=2)\n",
    "    del camera_timestamps_df['blank']\n",
    "    return camera_timestamps_df\n",
    "\n",
    "\n",
    "def convert_timestamp_to_seconds(timestamp: int) -> float:\n",
    "    \"\"\"\n",
    "    Converts the timestamp into seconds.\n",
    "\n",
    "    Args:\n",
    "        timestamp (int): The timestamp to be converted.\n",
    "\n",
    "    Returns:\n",
    "        float: The timestamp converted into seconds.\n",
    "    \"\"\"\n",
    "    cycle1 = (timestamp >> 12) & 0x1FFF\n",
    "    cycle2 = (timestamp >> 25) & 0x7F\n",
    "    time_in_seconds = cycle2 + cycle1 / 8000.0\n",
    "    return time_in_seconds\n",
    "\n",
    "\n",
    "def uncycle_timestamps(time_array: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Uncycles the time array.\n",
    "\n",
    "    Args:\n",
    "        time_array (np.ndarray): The time array to be uncycled.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The uncycled time array.\n",
    "    \"\"\"\n",
    "    cycles = np.insert(np.diff(time_array) < 0, 0, False)\n",
    "    cycle_index = np.cumsum(cycles)\n",
    "    return time_array + cycle_index * 128\n",
    "\n",
    "\n",
    "def convert_and_uncycle_timestamps(camera_timestamps_df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts the timestamps into seconds and then uncycles them.\n",
    "\n",
    "    Args:\n",
    "        camera_timestamps_df (pd.DataFrame): DataFrame containing camera timestamps.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Uncycled timestamps in seconds.\n",
    "    \"\"\"\n",
    "    timestamps_in_seconds = []\n",
    "    for index, row in camera_timestamps_df.iterrows():\n",
    "        if row.Trigger > 0: \n",
    "            timestamp_in_seconds = convert_timestamp_to_seconds(camera_timestamps_df.at[index, 'Timestamp'])\n",
    "            timestamps_in_seconds.append(timestamp_in_seconds)\n",
    "        else:    \n",
    "            raise ValueError('Timestamps are broken')\n",
    "\n",
    "    uncycled_timestamps = uncycle_timestamps(timestamps_in_seconds)\n",
    "    uncycled_timestamps = uncycled_timestamps - uncycled_timestamps[0]  # make first timestamp 0 and the others relative to this \n",
    "    return uncycled_timestamps\n",
    "\n",
    "\n",
    "def check_for_dropped_frames(timestamps: np.ndarray, expected_frame_rate: int) -> None:\n",
    "    \"\"\"\n",
    "    Checks for dropped frames in the timestamps.\n",
    "\n",
    "    Args:\n",
    "        timestamps (np.ndarray): The array of timestamps.\n",
    "        expected_frame_rate (int): The expected frame rate in frames per second.\n",
    "    \"\"\"\n",
    "    frame_gaps = 1 / np.diff(timestamps)\n",
    "    dropped_frames_count = np.sum((frame_gaps < expected_frame_rate - 5) | (frame_gaps > expected_frame_rate + 5))\n",
    "    \n",
    "    print(f'Frames dropped = {dropped_frames_count}')\n",
    "    plt.suptitle(f'Frame rate = {expected_frame_rate}fps', color = 'red')\n",
    "    plt.hist(frame_gaps, bins=100)\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Number of frames')\n",
    "\n",
    "\n",
    "def determine_trigger_states_from_raw_timestamps(raw_camera_timestamps_df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Determines the trigger states from the raw camera timestamps.\n",
    "\n",
    "    Args:\n",
    "        raw_camera_timestamps_df (pd.DataFrame): DataFrame containing raw camera timestamps.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of trigger states.\n",
    "    \"\"\"\n",
    "    down_state = raw_camera_timestamps_df['Trigger'][0]\n",
    "    down_state_times = np.where(raw_camera_timestamps_df['Trigger'] == down_state)\n",
    "    temporary_trigger_states = np.ones(len(raw_camera_timestamps_df['Trigger']))\n",
    "    temporary_trigger_states[down_state_times] = 0\n",
    "    return temporary_trigger_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### transition data preprocessing ###\n",
    "### ---------------------------- ###\n",
    "\n",
    "def determine_transition_times_and_types(all_port_in_times_sorted: np.ndarray,\n",
    "                                         all_port_out_times_sorted: np.ndarray,\n",
    "                                         all_port_references_sorted: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, List, List]:\n",
    "    \"\"\"\n",
    "    Determines transition times and types given sorted port in/out times and sorted port references.\n",
    "\n",
    "    Args:\n",
    "        all_port_in_times_sorted (np.ndarray): Array of sorted port in times.\n",
    "        all_port_out_times_sorted (np.ndarray): Array of sorted port out times.\n",
    "        all_port_references_sorted (np.ndarray): Array of sorted port references.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, List, List]: Tuple containing arrays of out-in transitions, in-in transitions, \n",
    "        transition types, and lists of out-in and in-in transition references.\n",
    "    \"\"\"\n",
    "    \n",
    "    out_in_transitions = []\n",
    "    in_in_transitions = []\n",
    "    transition_types = []\n",
    "    out_in_transition_references = []\n",
    "    in_in_transition_references = []\n",
    "\n",
    "    for index, port in enumerate(all_port_references_sorted):\n",
    "        if index > 0:\n",
    "            # Calculate out-in transition and reference\n",
    "            out_in_transitions.append(all_port_in_times_sorted[index] - all_port_out_times_sorted[index-1])\n",
    "            out_in_transition_references.append(all_port_out_times_sorted[index-1])\n",
    "            \n",
    "            # Calculate in-in transition and reference\n",
    "            in_in_transitions.append(all_port_in_times_sorted[index] - all_port_in_times_sorted[index-1])\n",
    "            in_in_transition_references.append(all_port_in_times_sorted[index-1])\n",
    "            \n",
    "            # Determine transition type\n",
    "            transition_types.append(int(str(all_port_references_sorted[index-1]) + str(port)))\n",
    "\n",
    "    return (np.array(out_in_transitions), np.array(in_in_transitions), np.array(transition_types), out_in_transition_references, in_in_transition_references)\n",
    "\n",
    "def get_start_end_port_id(transition_types: np.ndarray, start_end_arg: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Returns the start or end port id from the transition types.\n",
    "\n",
    "    Args:\n",
    "        transition_types (np.ndarray): Array of transition types.\n",
    "        start_end_arg (int): Indicator of whether to return start or end port id.\n",
    "                             0 for start, 1 for end.\n",
    "\n",
    "    Returns:\n",
    "        List[int]: List of start or end port ids depending on the start_end_arg.\n",
    "    \"\"\"\n",
    "    \n",
    "    output_ids = []\n",
    "    for transition in transition_types:\n",
    "        transition_str = str(transition)\n",
    "        output_ids.append(int(transition_str[start_end_arg]))\n",
    "\n",
    "    return output_ids\n",
    "\n",
    "def determine_repeat_port_events(start_port_ids: List[int], end_port_ids: List[int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Determines repeat port events.\n",
    "\n",
    "    Args:\n",
    "        start_port_ids (List[int]): List of start port ids.\n",
    "        end_port_ids (List[int]): List of end port ids.\n",
    "\n",
    "    Returns:\n",
    "        List[int]: List of flags indicating if a port is repeated. \n",
    "                   0 for repeated port and 1 for non-repeated.\n",
    "    \"\"\"\n",
    "    \n",
    "    port_repeat_flags = []\n",
    "    for start_id, end_id in zip(start_port_ids, end_port_ids):\n",
    "        if start_id == end_id:\n",
    "            port_repeat_flags.append(0)\n",
    "        else: \n",
    "            port_repeat_flags.append(1)\n",
    "\n",
    "    return port_repeat_flags\n",
    "\n",
    "def filter_transitions_by_latency(transition_times: List[float], upper_limit: float) -> List[int]:\n",
    "    \"\"\"\n",
    "    Filters transitions based on their latency times.\n",
    "\n",
    "    Args:\n",
    "        transition_times (List[float]): List of transition times.\n",
    "        upper_limit (float): The upper limit to filter the transitions.\n",
    "\n",
    "    Returns:\n",
    "        List[int]: List of flags indicating whether a transition time is less than the upper limit. \n",
    "                   1 for less than the limit and 0 otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    filtered_transitions = []\n",
    "    for time in transition_times:\n",
    "        if time < upper_limit:\n",
    "            filtered_transitions.append(1)\n",
    "        else:\n",
    "            filtered_transitions.append(0)\n",
    "\n",
    "    return filtered_transitions\n",
    "\n",
    "def calculate_port_events_in_camera_time(trial_start_timestamps: List[float], start_port_times: List[float], camera_start_timestamps: List[float]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Calculate the camera timestamps for port events.\n",
    "\n",
    "    Args:\n",
    "        trial_start_timestamps (List[float]): List of trial start timestamps.\n",
    "        start_port_times (List[float]): List of start port times.\n",
    "        camera_start_timestamps (List[float]): List of camera start timestamps.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: List of port events in camera time.\n",
    "    \"\"\"\n",
    "    \n",
    "    port_camera_timestamps = []\n",
    "    for index, start_time in enumerate(trial_start_timestamps[:-1]):\n",
    "        time_difference = start_port_times[index] - start_time\n",
    "        port_camera_timestamps.append(camera_start_timestamps[index] + time_difference)\n",
    "\n",
    "    return port_camera_timestamps\n",
    "\n",
    "def create_sequences_by_time_and_port(\n",
    "    transition_types: List[int],\n",
    "    transition_times: List[float],\n",
    "    port1: int,\n",
    "    transition_reference_time: List[float],\n",
    "    transition_filter_time: float\n",
    ") -> Tuple[List[List[int]], List[List[float]], List[List[float]]]:\n",
    "    \"\"\"\n",
    "    Reorder transitions into sequences relevant to time and port events. \n",
    "\n",
    "    Args:\n",
    "        transition_types (List[int]): List of transition types.\n",
    "        transition_times (List[float]): List of transition times.\n",
    "        port1 (int): Identifier for port1.\n",
    "        transition_reference_time (List[float]): List of reference times for transitions.\n",
    "        transition_filter_time (float): Time filter for transitions.\n",
    "\n",
    "    Returns:\n",
    "        TimeFiltered_ids (List[List[int]]): Nested list of filtered transition ids.\n",
    "        TimeFiltered_times (List[List[float]]): Nested list of filtered transition times.\n",
    "        Reference_times (List[List[float]]): Nested list of reference times for the filtered transitions.\n",
    "    \"\"\"\n",
    "    \n",
    "    sequence_index = 0\n",
    "    filtered_transition_ids = [[]]\n",
    "    filtered_transition_times = [[]]\n",
    "    reference_times = [[]]\n",
    "\n",
    "    for index, transition in enumerate(transition_types):\n",
    "        if 0.03 < transition_times[index] < transition_filter_time:  # if less than filter time and more than lower bound filter time (0.1s)\n",
    "            if int(str(transition)[0]) == port1:  # check if first port matches filter port\n",
    "                if filtered_transition_ids[sequence_index]:\n",
    "                    sequence_index += 1\n",
    "                    filtered_transition_ids.append([])\n",
    "                    filtered_transition_times.append([])\n",
    "                    reference_times.append([])\n",
    "                filtered_transition_ids[sequence_index].append(transition)\n",
    "                filtered_transition_times[sequence_index].append(transition_times[index])\n",
    "                reference_times[sequence_index].append(transition_reference_time[index])\n",
    "            else: \n",
    "                filtered_transition_ids[sequence_index].append(transition)\n",
    "                filtered_transition_times[sequence_index].append(transition_times[index])\n",
    "                reference_times[sequence_index].append(transition_reference_time[index])\n",
    "\n",
    "        elif filtered_transition_ids[sequence_index]:  # if not empty \n",
    "            sequence_index += 1\n",
    "            filtered_transition_ids.append([])\n",
    "            filtered_transition_times.append([])\n",
    "            reference_times.append([])\n",
    "\n",
    "    return filtered_transition_ids, filtered_transition_times, reference_times\n",
    "\n",
    "def number_of_rewarded_events(aligned_reward_timestamps: List) -> int:\n",
    "    \"\"\"\n",
    "    Function to count the number of non-NaN items in a list.\n",
    "\n",
    "    Parameters:\n",
    "    aligned_reward_timestamps (List): A list of numerical values or strings that may include NaNs.\n",
    "\n",
    "    Returns:\n",
    "    int: The count of non-NaN items in the list.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Use a generator expression with the sum function to count the non-NaN items\n",
    "    # The isinstance() function checks if an item is a float or integer\n",
    "    # The math.isnan() function checks if a numerical item is a NaN\n",
    "    return sum(1 for item in aligned_reward_timestamps if isinstance(item, (float, int)) and not math.isnan(item))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are the variable that would be obtained from the .json file from the command line in the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_ids = [\"EJT244\",\"SP110\", \"SP111\"]\n",
    "input_directory = '/home/sthitapati/Documents/sequence_data/bpod_raw_data/'\n",
    "output_directory = '/home/sthitapati/Documents/sequence_data/output/'\n",
    "camera_directory =  '/home/sthitapati/data/bpod_raw_data/EJT_FlyCap_SeqTracking/'\n",
    "replace_existing = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function to call all the functions and generate the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_animal_data(\n",
    "    animal_ids: List[str], \n",
    "    input_directory: str, \n",
    "    output_directory: str, \n",
    "    camera_directory: Optional[str] = None, \n",
    "    replace_existing: bool = False\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Function to process data for each animal and each session.\n",
    "\n",
    "    Args:\n",
    "        animal_ids (List[str]): List of animal IDs.\n",
    "        input_directory (str): Directory containing raw behavioral data for each animal.\n",
    "        output_directory (str): Directory where processed data will be saved.\n",
    "        camera_directory (Optional[str]): Directory containing the camera timestamp files for each animal, if available.\n",
    "        replace_existing (bool): If True, existing processed data will be replaced. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate over each animal by its index and ID\n",
    "    for animal_index, current_animal_id in enumerate(animal_ids):\n",
    "        print ('Processing data for: ' + current_animal_id)\n",
    "\n",
    "        # Construct the path for the current animal's data\n",
    "        current_input_path = os.path.join(input_directory, current_animal_id, 'Sequence_Automated', 'Session Data/')\n",
    "\n",
    "        # Load Behavioural data using the import_bpod_data_files function\n",
    "        behavior_data, total_sessions, path, session_dates = import_bpod_data_files(current_input_path)\n",
    "\n",
    "        # Initialize strings to store processed and skipped sessions\n",
    "        processed_sessions = ''\n",
    "        skipped_sessions = ''\n",
    "\n",
    "        # Iterate over each session\n",
    "        for session_index in range(total_sessions):\n",
    "\n",
    "            # Create unique identifier for the session\n",
    "            session_date = session_dates[session_index] + '_' + str(behavior_data[session_index]['__header__'])[-25:-22]\n",
    "\n",
    "            # Set the save path depending on the session number\n",
    "            if session_index < 10:\n",
    "                save_path = os.path.join(output_directory, current_animal_id, 'Preprocessed', f'0{session_index}_{session_date}')\n",
    "            else:\n",
    "                save_path = os.path.join(output_directory, current_animal_id, 'Preprocessed', f'{session_index}_{session_date}')\n",
    "\n",
    "            # Check if the directory exists already\n",
    "            if not os.path.isdir(save_path):\n",
    "                # If it doesn't exist, make the directory and set the processing flag to True\n",
    "                os.makedirs(save_path)\n",
    "                should_process = True\n",
    "            else:\n",
    "                # If it does exist, check the replace_existing flag to determine if data should be processed\n",
    "                should_process = replace_existing\n",
    "\n",
    "            # If processing flag is True, convert the data to a Python-friendly format\n",
    "            if should_process:\n",
    "                # Calculate final reward amount for the session\n",
    "                final_reward_amounts = []\n",
    "                for item in behavior_data[session_index]['SessionData']['SessionVariables']['TLevel']:\n",
    "                    training_level = item\n",
    "                    final_reward_amounts.append(behavior_data[0]['SessionData']['SessionVariables']['TrainingLevels'][training_level-1][4])\n",
    "\n",
    "                # save out training levels on their own\n",
    "                filename = 'PreProcessed_TrainingLevels' \n",
    "                with open(save_path + '/'+ filename, 'wb') as fp:\n",
    "                    pickle.dump(behavior_data[session_index]['SessionData']['SessionVariables']['TLevel'], fp)\n",
    "\n",
    "                # fetch trial_settings \n",
    "                trial_settings = behavior_data[session_index]['SessionData']['TrialSettings'][0]\n",
    "\n",
    "\n",
    "                # Save out LED intensities and reward amounts on their own:\n",
    "                led_intensities = pd.DataFrame({\n",
    "                    'Port2': behavior_data[session_index]['SessionData']['SessionVariables']['LEDIntensitys']['port2'],\n",
    "                    'Port3': behavior_data[session_index]['SessionData']['SessionVariables']['LEDIntensitys']['port3'],\n",
    "                    'Port4': behavior_data[session_index]['SessionData']['SessionVariables']['LEDIntensitys']['port4'],\n",
    "                    'Port5': behavior_data[session_index]['SessionData']['SessionVariables']['LEDIntensitys']['port5']\n",
    "                })\n",
    "\n",
    "                # Save out LED intensities and reward amounts on their own:\n",
    "                led_intensities.to_csv(save_path + '/PreProcessed_LED_Intensities.csv')\n",
    "\n",
    "                # Create a DataFrame for reward amounts for each port:41\n",
    "                reward_amounts = pd.DataFrame({\n",
    "                    'Port1': behavior_data[session_index]['SessionData']['SessionVariables']['RewardAmount']['port1'],\n",
    "                    'Port2': behavior_data[session_index]['SessionData']['SessionVariables']['RewardAmount']['port2'],\n",
    "                    'Port3': behavior_data[session_index]['SessionData']['SessionVariables']['RewardAmount']['port3'],\n",
    "                    'Port4': behavior_data[session_index]['SessionData']['SessionVariables']['RewardAmount']['port4']\n",
    "                })\n",
    "                \n",
    "                # Save out reward amounts on their own:\n",
    "                reward_amounts.to_csv(save_path + '/PreProcessed_Reward_Amounts.csv')\n",
    "\n",
    "                # Extract PortIn times for each port and check for errors\n",
    "                port_in_times, port_out_times, port_references = extract_poke_times(behavior_data[session_index])\n",
    "\n",
    "                # Remove 'nan' values (these represent times when part of the event was dropped by Bpod for some reason)\n",
    "                fixed_port_in_times, fixed_port_out_times, fixed_port_references = remove_dropped_in_events(port_in_times, port_out_times, port_references)\n",
    "\n",
    "                # Resort these times for consistent chronology\n",
    "                sorted_port_in_times, sorted_port_out_times, sorted_port_references = sort_by_time(fixed_port_in_times, fixed_port_out_times, fixed_port_references)\n",
    "\n",
    "                \n",
    "                # Extract reward timestamps:\n",
    "                reward_timestamps = extract_reward_timestamps(behavior_data[session_index])\n",
    "\n",
    "\n",
    "                # Find indices corresponding to rewarded events and align them to poke events:\n",
    "                rewarded_event_indices = find_rewarded_event_indices(sorted_port_in_times, sorted_port_references, reward_timestamps)\n",
    "\n",
    "                # Remove 'NaN' entries from reward timestamps:\n",
    "                reward_timestamps = np.asarray(reward_timestamps)\n",
    "                reward_timestamps = reward_timestamps[np.logical_not(np.isnan(reward_timestamps))]\n",
    "                reward_timestamps = list(reward_timestamps)\n",
    "\n",
    "                # Align reward timestamps to the corresponding poke events:\n",
    "                aligned_reward_timestamps = align_trigger_to_index(reward_timestamps, rewarded_event_indices, sorted_port_references)\n",
    "\n",
    "                # Extract trial start timestamps:\n",
    "                trial_start_timestamps = extract_trial_timestamps(behavior_data[session_index])\n",
    "\n",
    "                # Extract trial end times:\n",
    "                trial_end_timestamps = extract_trial_end_times(behavior_data[session_index])\n",
    "\n",
    "                # Determine trial IDs:\n",
    "                trial_ids = determine_trial_id(sorted_port_in_times, trial_end_timestamps)\n",
    "\n",
    "                # Find trial start indices:\n",
    "                trial_start_indices = find_trial_start_indices(trial_ids)\n",
    "\n",
    "                # Align trial start timestamps to poke events:\n",
    "                aligned_trial_start_timestamps = align_trial_start_end_timestamps(trial_ids, trial_start_indices, trial_start_timestamps)\n",
    "\n",
    "                # Align trial end timestamps to poke events:\n",
    "                aligned_trial_end_timestamps = align_trial_start_end_timestamps(trial_ids, trial_start_indices, trial_end_timestamps)\n",
    "                \n",
    "                # handle optogenetic stimulation\n",
    "                optotrials_aligned, optotrials_port_aligned = handle_opto_stim_data(behavior_data, trial_settings, session_index, trial_ids)\n",
    "\n",
    "                # Create empty lists to store intermediate rewards and LED intensities data for each trial\n",
    "                intermediate_rewards_data = []\n",
    "                led_intensities_data = []\n",
    "\n",
    "                # Iterate over 'TLevel' items in SessionVariables\n",
    "                for tlevel_item in behavior_data[session_index]['SessionData']['SessionVariables']['TLevel']:\n",
    "                    tlevel = tlevel_item\n",
    "                    # Append intermediate rewards and LED intensities data for the current trial\n",
    "                    intermediate_rewards_data.append(\n",
    "                        list(behavior_data[session_index]['SessionData']['SessionVariables']['TrainingLevels'][tlevel-1][0:4])\n",
    "                    )\n",
    "                    led_intensities_data.append(\n",
    "                        list(behavior_data[session_index]['SessionData']['SessionVariables']['TrainingLevels'][tlevel-1][6:10])\n",
    "                    )\n",
    "\n",
    "                # Align intermediate rewards and LED intensities data with trial start indices\n",
    "                aligned_led_intensities = align_trial_start_end_timestamps(trial_ids, trial_start_indices, led_intensities_data)\n",
    "                aligned_intermediate_rewards = align_trial_start_end_timestamps(trial_ids, trial_start_indices, intermediate_rewards_data)\n",
    "\n",
    "\n",
    "                # Align training level for each trial\n",
    "                training_levels = align_data_to_trial_ids(trial_ids, behavior_data[session_index]['SessionData']['SessionVariables']['TLevel'])\n",
    "\n",
    "                # Process camera timestamps for a specific animal and session\n",
    "                do_timestamps_exist, aligned_start_trial_camera_timestamps, aligned_end_trial_camera_timestamps, aligned_first_poke_camera_timestamps = handle_camera_data(session_date, camera_directory, \n",
    "                                                                                                                                                    current_animal_id, trial_ids, \n",
    "                                                                                                                                                    trial_start_indices, sorted_port_references, save_path)\n",
    "\n",
    "\n",
    "                PortIn_df = pd.DataFrame(\n",
    "                    {\n",
    "                        'trial_id': trial_ids,\n",
    "                        'trial_start_time': aligned_trial_start_timestamps,\n",
    "                        'poke_port': sorted_port_references,\n",
    "                        'poke_in_timestamp': sorted_port_in_times,\n",
    "                        'poke_out_timestamp': sorted_port_out_times,\n",
    "                        'reward_timestamps': aligned_reward_timestamps,\n",
    "                        'trial_end_time': aligned_trial_end_timestamps,\n",
    "                        'trial_start_camera_timestamp': aligned_start_trial_camera_timestamps,\n",
    "                        'trial_end_camera_timestamp': aligned_end_trial_camera_timestamps,\n",
    "                        'first_poke_camera_timestamp': aligned_first_poke_camera_timestamps,\n",
    "                        'led_intensities_ports_2_3_4_5': aligned_led_intensities,\n",
    "                        'reward_amounts_ports_1_2_3_4': aligned_intermediate_rewards,\n",
    "                        'opto_condition': optotrials_aligned,\n",
    "                        'opto_stimulated_port': optotrials_port_aligned,\n",
    "                        'training_level': training_levels\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                \n",
    "                #Save Data\n",
    "                PortIn_df.to_csv(save_path +'/PreProcessed_RawPokeData.csv')\n",
    "\n",
    "                # PART 2: Transitions\n",
    "                # Determine Transition times and types for all events \n",
    "                out_in_transition_times, in_in_transition_times, transition_types, out_in_transition_reference, in_in_transition_reference = determine_transition_times_and_types(sorted_port_in_times, sorted_port_out_times, sorted_port_references)\n",
    "\n",
    "                # Split transition types into first and last ports: \n",
    "                start_port_ids = get_start_end_port_id(transition_types, 0)\n",
    "                end_port_ids = get_start_end_port_id(transition_types, 1)\n",
    "\n",
    "                # Align start and end port times\n",
    "                end_port_in_time = sorted_port_in_times[1:]\n",
    "                start_port_in_time = sorted_port_in_times[:-1]\n",
    "                end_port_out_time = sorted_port_out_times[1:]\n",
    "                start_port_out_time = sorted_port_out_times[:-1]\n",
    "\n",
    "                # Find Port repeat events (double pokes)\n",
    "                non_port_repeat = determine_repeat_port_events(start_port_ids, end_port_ids)\n",
    "\n",
    "                # Determine which transitions are good: less than 2s\n",
    "                out_in_filtered_transitions = filter_transitions_by_latency(out_in_transition_times, upper_limit=2)\n",
    "                in_in_filtered_transitions = filter_transitions_by_latency(in_in_transition_times, upper_limit=2)\n",
    "\n",
    "                if do_timestamps_exist:\n",
    "                    # Align camera timestamps to each transition event \n",
    "                    first_port_camera_ts = calculate_port_events_in_camera_time(aligned_start_trial_camera_timestamps, start_port_in_time, aligned_end_trial_camera_timestamps)\n",
    "                    second_port_camera_ts = first_port_camera_ts + in_in_transition_times\n",
    "                else:\n",
    "                    first_port_camera_ts = ['NaN'] * len(transition_types)\n",
    "                    second_port_camera_ts = ['NaN'] * len(transition_types)\n",
    "\n",
    "                # Create DataFrame\n",
    "                transition_df = pd.DataFrame(\n",
    "                    {\n",
    "                        'trial_id': trial_ids[:-1],\n",
    "                        'transition_type': transition_types,\n",
    "                        'start_poke_port': start_port_ids,\n",
    "                        'end_poke_port': end_port_ids,\n",
    "                        'start_poke_in_timestamp': start_port_in_time,\n",
    "                        'start_poke_out_timestamp': start_port_out_time,\n",
    "                        'end_poke_in_timestamp': end_port_in_time,\n",
    "                        'end_poke_out_timestamp': end_port_out_time,\n",
    "                        'out_in_latency': out_in_transition_times,\n",
    "                        'in_in_latency': in_in_transition_times,\n",
    "                        'first_poke_camera_timestamp': first_port_camera_ts,\n",
    "                        'second_poke_camera_timestamp': second_port_camera_ts,\n",
    "                        'repeat_filter': non_port_repeat,\n",
    "                        '2s_time_filter_out_in': out_in_filtered_transitions,\n",
    "                        '2s_time_filter_in_in': in_in_filtered_transitions,\n",
    "                        'opto_condition': optotrials_aligned[:-1],\n",
    "                        'opto_stimulated_port': optotrials_port_aligned[:-1],\n",
    "                        'training_level': training_levels[:-1],\n",
    "                        'led_intensities_ports_2_3_4_5': aligned_led_intensities[:-1],\n",
    "                        'reward_amounts_ports_1_2_3_4': aligned_intermediate_rewards[:-1]\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Save Data\n",
    "                transition_df.to_csv(os.path.join(save_path, 'PreProcessed_TransitionData.csv'))\n",
    "\n",
    "\n",
    "\n",
    "                # Define useful port/sequence related information\n",
    "                port1 = behavior_data[session_index]['SessionData']['TrialSequence'][0][0]\n",
    "                port2 = behavior_data[session_index]['SessionData']['TrialSequence'][0][1]\n",
    "                port3 = behavior_data[session_index]['SessionData']['TrialSequence'][0][2]\n",
    "                port4 = behavior_data[session_index]['SessionData']['TrialSequence'][0][3]\n",
    "                port5 = behavior_data[session_index]['SessionData']['TrialSequence'][0][4]\n",
    "\n",
    "                sequence1 = int(f\"{port1}{port2}\")\n",
    "                sequence2 = int(f\"{port2}{port3}\")\n",
    "                sequence3 = int(f\"{port3}{port4}\")\n",
    "                sequence4 = int(f\"{port4}{port5}\")\n",
    "\n",
    "                # Filter transitions into sequences for each port\n",
    "                transition_filter_time = 2.0\n",
    "                port1_time_filtered_ids, port1_time_filtered_times, port1_ref_times = create_sequences_by_time_and_port(\n",
    "                    transition_types, in_in_transition_times, port1, in_in_transition_reference, transition_filter_time\n",
    "                )\n",
    "\n",
    "                port2_time_filtered_ids, port2_time_filtered_times, port2_ref_times = create_sequences_by_time_and_port(\n",
    "                    transition_types, in_in_transition_times, port2, in_in_transition_reference, transition_filter_time\n",
    "                )\n",
    "\n",
    "                port3_time_filtered_ids, port3_time_filtered_times, port3_ref_times = create_sequences_by_time_and_port(\n",
    "                    transition_types, in_in_transition_times, port3, in_in_transition_reference, transition_filter_time\n",
    "                )\n",
    "\n",
    "                port4_time_filtered_ids, port4_time_filtered_times, port4_ref_times = create_sequences_by_time_and_port(\n",
    "                    transition_types, in_in_transition_times, port4, in_in_transition_reference, transition_filter_time\n",
    "                )\n",
    "\n",
    "                port5_time_filtered_ids, port5_time_filtered_times, port5_ref_times = create_sequences_by_time_and_port(\n",
    "                    transition_types, in_in_transition_times, port5, in_in_transition_reference, transition_filter_time\n",
    "                )\n",
    "\n",
    "                # Filter transitions into sequences that are within the transition filter time (not filtered to start at first poke):\n",
    "                time_filtered_ids, time_filtered_times, reference_times = create_sequences_by_time_and_port(\n",
    "                    transition_types, out_in_transition_times, port1, in_in_transition_reference, transition_filter_time\n",
    "                )\n",
    "\n",
    "                # Make dataframes\n",
    "                sequence_df_time_filtered_port1_aligned = pd.DataFrame({\n",
    "                    'sequence_ids': port1_time_filtered_ids,\n",
    "                    'sequence_times': port1_time_filtered_times,\n",
    "                    'session_time_reference': port1_ref_times\n",
    "                })\n",
    "\n",
    "                sequence_df_time_filtered_port2_aligned = pd.DataFrame({\n",
    "                    'sequence_ids': port2_time_filtered_ids,\n",
    "                    'sequence_times': port2_time_filtered_times,\n",
    "                    'session_time_reference': port2_ref_times\n",
    "                })\n",
    "\n",
    "                sequence_df_time_filtered_port3_aligned = pd.DataFrame({\n",
    "                    'sequence_ids': port3_time_filtered_ids,\n",
    "                    'sequence_times': port3_time_filtered_times,\n",
    "                    'session_time_reference': port3_ref_times\n",
    "                })\n",
    "\n",
    "                sequence_df_time_filtered_port4_aligned = pd.DataFrame({\n",
    "                    'sequence_ids': port4_time_filtered_ids,\n",
    "                    'sequence_times': port4_time_filtered_times,\n",
    "                    'session_time_reference': port4_ref_times\n",
    "                })\n",
    "\n",
    "                sequence_df_time_filtered_port5_aligned = pd.DataFrame({\n",
    "                    'sequence_ids': port5_time_filtered_ids,\n",
    "                    'sequence_times': port5_time_filtered_times,\n",
    "                    'session_time_reference': port5_ref_times\n",
    "                })\n",
    "\n",
    "                # Save Data\n",
    "                sequence_df_time_filtered_port1_aligned.to_csv(save_path + '/PreProcessed_Sequence_df_timefiltered_port1aligned.csv')\n",
    "                sequence_df_time_filtered_port2_aligned.to_csv(save_path + '/PreProcessed_Sequence_df_timefiltered_port2aligned.csv')\n",
    "                sequence_df_time_filtered_port3_aligned.to_csv(save_path + '/PreProcessed_Sequence_df_timefiltered_port3aligned.csv')\n",
    "                sequence_df_time_filtered_port4_aligned.to_csv(save_path + '/PreProcessed_Sequence_df_timefiltered_port4aligned.csv')\n",
    "                sequence_df_time_filtered_port5_aligned.to_csv(save_path + '/PreProcessed_Sequence_df_timefiltered_port5aligned.csv')\n",
    "\n",
    "                # Make final session information dataframe\n",
    "                training_levels = list(trial_settings['GUIMeta']['TrainingLevel']['String'])\n",
    "                session_level = trial_settings['GUI']['TrainingLevel']\n",
    "                no_rewarded_events = number_of_rewarded_events(aligned_reward_timestamps)\n",
    "\n",
    "                # experiment or training session:\n",
    "                if trial_settings['GUI']['ExperimentType'] == 2:\n",
    "                    experiment = 1\n",
    "                else:\n",
    "                    experiment = 0\n",
    "\n",
    "                session_information = pd.DataFrame({\n",
    "                    'port1': [port1],\n",
    "                    'port2': [port2],\n",
    "                    'port3': [port3],\n",
    "                    'port4': [port4],\n",
    "                    'port5': [port5],\n",
    "                    'transition1': sequence1,\n",
    "                    'transition2': sequence2,\n",
    "                    'transition3': sequence3,\n",
    "                    'transition4': sequence4,\n",
    "                    'n_trials': [trial_ids[-1]],\n",
    "                    'n_rewards': [no_rewarded_events],\n",
    "                    'final_reward_amount': [final_reward_amounts],\n",
    "                    'session_level': [training_levels],\n",
    "                    'experiment': [experiment],\n",
    "                    'camera_data': [do_timestamps_exist],\n",
    "                })\n",
    "\n",
    "\n",
    "                # Save Data\n",
    "                session_information.to_csv(save_path + '/PreProcessed_SessionInfo.csv')\n",
    "                processed_sessions = \", \".join(str(session) for session in processed_sessions)\n",
    "                        \n",
    "            else:\n",
    "                skipped_sessions = \", \".join(str(session) for session in skipped_sessions)\n",
    "\n",
    "\n",
    "        print(f'Already Processed so skipped: {skipped_sessions}')\n",
    "        print(f'Processed: {processed_sessions}')\n",
    "    print('finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for: EJT244\n",
      "Already Processed so skipped: \n",
      "Processed: \n",
      "Processing data for: SP110\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "Already Processed so skipped: \n",
      "Processed: \n",
      "Processing data for: SP111\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "No camera timestamps found for the given session.\n",
      "Already Processed so skipped: \n",
      "Processed: \n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "process_animal_data(animal_ids=animal_ids, input_directory=input_directory, output_directory=output_directory, camera_directory=camera_directory, replace_existing=replace_existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sarabhai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
